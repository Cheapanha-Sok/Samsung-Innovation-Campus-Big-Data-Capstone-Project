{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Wrangling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Import necessary modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import StructType, StructField, StringType, FloatType\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Configure the hadoop dir path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input path hdfs://localhost:9000/user/hadoop/inputs , merge path hdfs://localhost:9000/user/hadoop/merged\n"
     ]
    }
   ],
   "source": [
    "BASE_PATH = 'hdfs://localhost:9000/user/hadoop'\n",
    "INPUT_PATH = f'{BASE_PATH}/inputs'\n",
    "MERGED_PATH = f'{BASE_PATH}/merged'\n",
    "\n",
    "print('input path {} , merge path {}'.format(INPUT_PATH, MERGED_PATH))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Function for create spark session that connect to hadoop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_spark_session():\n",
    "    \"\"\"\n",
    "    Creates and configures a SparkSession with minimal memory settings.\n",
    "    \"\"\"\n",
    "    spark = SparkSession.builder \\\n",
    "    .appName(\"Weather Data Combination\") \\\n",
    "    .config(\"spark.jars.packages\", \"com.crealytics:spark-excel_2.12:0.13.7\") \\\n",
    "    .config(\"spark.driver.memory\", \"2g\") \\\n",
    "    .config(\"spark.executor.memory\", \"2g\") \\\n",
    "    .config(\"spark.driver.maxResultSize\", \"1g\") \\\n",
    "    .config(\"spark.ui.showConsoleProgress\", \"true\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "    return spark\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Create a function to load data from hadoop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_excel_files(spark, file_path):\n",
    "    \"\"\"\n",
    "    Load all Excel files from the given base path.\n",
    "    \"\"\"\n",
    "    df = spark.read.format(\"com.crealytics.spark.excel\") \\\n",
    "                .option(\"header\", \"false\") \\\n",
    "                .option(\"dataAddress\", \"'RUA Data'!A6\") \\\n",
    "                .option(\"maxRowsInMemory\", 1000) \\\n",
    "                .option(\"treatEmptyValuesAsNulls\", \"true\") \\\n",
    "                .option(\"inferSchema\", \"false\") \\\n",
    "                .load(file_path)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Test read data as xlsx file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/11/30 22:25:55 WARN SparkSession: Using an existing Spark session; only runtime SQL configurations will take effect.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------+--------+------+---+---+-----+----+---+---+----+-----+\n",
      "|_c0|     _c1|     _c2|   _c3|_c4|_c5|  _c6| _c7|_c8|_c9|_c10| _c11|\n",
      "+---+--------+--------+------+---+---+-----+----+---+---+----+-----+\n",
      "|  1|21/04/01|00:00:00|0.2534|  1|  0|28.12|80.6|  0|0.3| 149| 24.5|\n",
      "|  2|21/04/01|00:05:00|0.2532|  1|  0|28.02|  81|  0|  0| 215|24.49|\n",
      "|  3|21/04/01|00:10:00|0.2524|  1|  0|28.07|  81|  0|1.3| 170|24.53|\n",
      "|  4|21/04/01|00:15:00|0.2524|  1|  0| 28.1|80.8|  0|1.7| 166|24.52|\n",
      "|  5|21/04/01|00:20:00|0.2524|  1|  0|28.07|80.8|0.3|2.7| 181|24.49|\n",
      "+---+--------+--------+------+---+---+-----+----+---+---+----+-----+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark = create_spark_session()\n",
    "hdfs_path = f'{INPUT_PATH}/APRIL-2021.xlsx'\n",
    "df = load_excel_files(spark, hdfs_path)\n",
    "df.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Configure the files that we need to work with"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['hdfs://localhost:9000/user/hadoop/inputs/APRIL-2021.xlsx', 'hdfs://localhost:9000/user/hadoop/inputs/APRIL-2022.xlsx', 'hdfs://localhost:9000/user/hadoop/inputs/AUGUST-2021.xlsx', 'hdfs://localhost:9000/user/hadoop/inputs/DECEMBER-2020.xlsx', 'hdfs://localhost:9000/user/hadoop/inputs/DECEMBER-2021.xlsx', 'hdfs://localhost:9000/user/hadoop/inputs/FEBRUARY-2021.xlsx', 'hdfs://localhost:9000/user/hadoop/inputs/FEBRUARY-2022.xlsx', 'hdfs://localhost:9000/user/hadoop/inputs/JANUARY-2021.xlsx', 'hdfs://localhost:9000/user/hadoop/inputs/JANUARY-2022.xlsx', 'hdfs://localhost:9000/user/hadoop/inputs/JULY-2021.xlsx', 'hdfs://localhost:9000/user/hadoop/inputs/MARCH-2021.xlsx', 'hdfs://localhost:9000/user/hadoop/inputs/MARCH-2022.xlsx', 'hdfs://localhost:9000/user/hadoop/inputs/MAY-2021.xlsx', 'hdfs://localhost:9000/user/hadoop/inputs/NOVEMBER-2020.xlsx', 'hdfs://localhost:9000/user/hadoop/inputs/NOVEMBER-2021.xlsx', 'hdfs://localhost:9000/user/hadoop/inputs/OCTOBER-2020.xlsx', 'hdfs://localhost:9000/user/hadoop/inputs/OCTOBER-2021.xlsx', 'hdfs://localhost:9000/user/hadoop/inputs/SEPTEMBER-2020.xlsx', 'hdfs://localhost:9000/user/hadoop/inputs/SEPTEMBER-2021.xlsx', 'hdfs://localhost:9000/user/hadoop/inputs/jUNE-2021.xlsx']\n"
     ]
    }
   ],
   "source": [
    "file_paths = [f\"{INPUT_PATH}/{filename}\" for filename in [\n",
    "    \"APRIL-2021.xlsx\", \"APRIL-2022.xlsx\", \"AUGUST-2021.xlsx\",\n",
    "    \"DECEMBER-2020.xlsx\", \"DECEMBER-2021.xlsx\", \"FEBRUARY-2021.xlsx\",\n",
    "    \"FEBRUARY-2022.xlsx\", \"JANUARY-2021.xlsx\", \"JANUARY-2022.xlsx\",\n",
    "    \"JULY-2021.xlsx\", \"MARCH-2021.xlsx\", \"MARCH-2022.xlsx\",\n",
    "    \"MAY-2021.xlsx\", \"NOVEMBER-2020.xlsx\", \"NOVEMBER-2021.xlsx\",\n",
    "    \"OCTOBER-2020.xlsx\", \"OCTOBER-2021.xlsx\", \"SEPTEMBER-2020.xlsx\",\n",
    "    \"SEPTEMBER-2021.xlsx\", \"jUNE-2021.xlsx\"\n",
    "]]\n",
    "print(file_paths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing file 1/20: hdfs://localhost:9000/user/hadoop/inputs/APRIL-2021.xlsx\n",
      "Successfully processed hdfs://localhost:9000/user/hadoop/inputs/APRIL-2021.xlsx\n",
      "Processing file 2/20: hdfs://localhost:9000/user/hadoop/inputs/APRIL-2022.xlsx\n",
      "Successfully processed hdfs://localhost:9000/user/hadoop/inputs/APRIL-2022.xlsx\n",
      "Processing file 3/20: hdfs://localhost:9000/user/hadoop/inputs/AUGUST-2021.xlsx\n",
      "Successfully processed hdfs://localhost:9000/user/hadoop/inputs/AUGUST-2021.xlsx\n",
      "Processing file 4/20: hdfs://localhost:9000/user/hadoop/inputs/DECEMBER-2020.xlsx\n",
      "Successfully processed hdfs://localhost:9000/user/hadoop/inputs/DECEMBER-2020.xlsx\n",
      "Processing file 5/20: hdfs://localhost:9000/user/hadoop/inputs/DECEMBER-2021.xlsx\n",
      "Successfully processed hdfs://localhost:9000/user/hadoop/inputs/DECEMBER-2021.xlsx\n",
      "Processing file 6/20: hdfs://localhost:9000/user/hadoop/inputs/FEBRUARY-2021.xlsx\n",
      "Successfully processed hdfs://localhost:9000/user/hadoop/inputs/FEBRUARY-2021.xlsx\n",
      "Processing file 7/20: hdfs://localhost:9000/user/hadoop/inputs/FEBRUARY-2022.xlsx\n",
      "Successfully processed hdfs://localhost:9000/user/hadoop/inputs/FEBRUARY-2022.xlsx\n",
      "Processing file 8/20: hdfs://localhost:9000/user/hadoop/inputs/JANUARY-2021.xlsx\n",
      "Successfully processed hdfs://localhost:9000/user/hadoop/inputs/JANUARY-2021.xlsx\n",
      "Processing file 9/20: hdfs://localhost:9000/user/hadoop/inputs/JANUARY-2022.xlsx\n",
      "Successfully processed hdfs://localhost:9000/user/hadoop/inputs/JANUARY-2022.xlsx\n",
      "Processing file 10/20: hdfs://localhost:9000/user/hadoop/inputs/JULY-2021.xlsx\n",
      "Successfully processed hdfs://localhost:9000/user/hadoop/inputs/JULY-2021.xlsx\n",
      "Processing file 11/20: hdfs://localhost:9000/user/hadoop/inputs/MARCH-2021.xlsx\n",
      "Successfully processed hdfs://localhost:9000/user/hadoop/inputs/MARCH-2021.xlsx\n",
      "Processing file 12/20: hdfs://localhost:9000/user/hadoop/inputs/MARCH-2022.xlsx\n",
      "Successfully processed hdfs://localhost:9000/user/hadoop/inputs/MARCH-2022.xlsx\n",
      "Processing file 13/20: hdfs://localhost:9000/user/hadoop/inputs/MAY-2021.xlsx\n",
      "Successfully processed hdfs://localhost:9000/user/hadoop/inputs/MAY-2021.xlsx\n",
      "Processing file 14/20: hdfs://localhost:9000/user/hadoop/inputs/NOVEMBER-2020.xlsx\n",
      "Error processing hdfs://localhost:9000/user/hadoop/inputs/NOVEMBER-2020.xlsx: [NUM_COLUMNS_MISMATCH] UNION can only be performed on inputs with the same number of columns, but the first input has 12 columns and the 14th input has 13 columns.;\n",
      "'Union false, false\n",
      ":- Relation [_c0#3683,_c1#3684,_c2#3685,_c3#3686,_c4#3687,_c5#3688,_c6#3689,_c7#3690,_c8#3691,_c9#3692,_c10#3693,_c11#3694] ExcelRelation(com.crealytics.spark.excel.CellRangeAddressDataLocator@5daaceb9,false,true,false,false,false,false,None,None,10,com.crealytics.spark.excel.StreamingWorkbookReader@53675641)\n",
      ":- Relation [_c0#3707,_c1#3708,_c2#3709,_c3#3710,_c4#3711,_c5#3712,_c6#3713,_c7#3714,_c8#3715,_c9#3716,_c10#3717,_c11#3718] ExcelRelation(com.crealytics.spark.excel.CellRangeAddressDataLocator@417e0ef3,false,true,false,false,false,false,None,None,10,com.crealytics.spark.excel.StreamingWorkbookReader@3ab5d399)\n",
      ":- Relation [_c0#3743,_c1#3744,_c2#3745,_c3#3746,_c4#3747,_c5#3748,_c6#3749,_c7#3750,_c8#3751,_c9#3752,_c10#3753,_c11#3754] ExcelRelation(com.crealytics.spark.excel.CellRangeAddressDataLocator@31e10d70,false,true,false,false,false,false,None,None,10,com.crealytics.spark.excel.StreamingWorkbookReader@585d9053)\n",
      ":- Relation [_c0#3779,_c1#3780,_c2#3781,_c3#3782,_c4#3783,_c5#3784,_c6#3785,_c7#3786,_c8#3787,_c9#3788,_c10#3789,_c11#3790] ExcelRelation(com.crealytics.spark.excel.CellRangeAddressDataLocator@69e5143d,false,true,false,false,false,false,None,None,10,com.crealytics.spark.excel.StreamingWorkbookReader@7d582b83)\n",
      ":- Relation [_c0#3815,_c1#3816,_c2#3817,_c3#3818,_c4#3819,_c5#3820,_c6#3821,_c7#3822,_c8#3823,_c9#3824,_c10#3825,_c11#3826] ExcelRelation(com.crealytics.spark.excel.CellRangeAddressDataLocator@446b33d5,false,true,false,false,false,false,None,None,10,com.crealytics.spark.excel.StreamingWorkbookReader@7080ec03)\n",
      ":- Relation [_c0#3851,_c1#3852,_c2#3853,_c3#3854,_c4#3855,_c5#3856,_c6#3857,_c7#3858,_c8#3859,_c9#3860,_c10#3861,_c11#3862] ExcelRelation(com.crealytics.spark.excel.CellRangeAddressDataLocator@d2c4c47,false,true,false,false,false,false,None,None,10,com.crealytics.spark.excel.StreamingWorkbookReader@37ec87b6)\n",
      ":- Relation [_c0#3887,_c1#3888,_c2#3889,_c3#3890,_c4#3891,_c5#3892,_c6#3893,_c7#3894,_c8#3895,_c9#3896,_c10#3897,_c11#3898] ExcelRelation(com.crealytics.spark.excel.CellRangeAddressDataLocator@3fd56dea,false,true,false,false,false,false,None,None,10,com.crealytics.spark.excel.StreamingWorkbookReader@6fd10ecd)\n",
      ":- Relation [_c0#3923,_c1#3924,_c2#3925,_c3#3926,_c4#3927,_c5#3928,_c6#3929,_c7#3930,_c8#3931,_c9#3932,_c10#3933,_c11#3934] ExcelRelation(com.crealytics.spark.excel.CellRangeAddressDataLocator@47809630,false,true,false,false,false,false,None,None,10,com.crealytics.spark.excel.StreamingWorkbookReader@3271f6d)\n",
      ":- Relation [_c0#3959,_c1#3960,_c2#3961,_c3#3962,_c4#3963,_c5#3964,_c6#3965,_c7#3966,_c8#3967,_c9#3968,_c10#3969,_c11#3970] ExcelRelation(com.crealytics.spark.excel.CellRangeAddressDataLocator@31769d1a,false,true,false,false,false,false,None,None,10,com.crealytics.spark.excel.StreamingWorkbookReader@4450b4c9)\n",
      ":- Relation [_c0#3995,_c1#3996,_c2#3997,_c3#3998,_c4#3999,_c5#4000,_c6#4001,_c7#4002,_c8#4003,_c9#4004,_c10#4005,_c11#4006] ExcelRelation(com.crealytics.spark.excel.CellRangeAddressDataLocator@2a7f4349,false,true,false,false,false,false,None,None,10,com.crealytics.spark.excel.StreamingWorkbookReader@5d86dc0d)\n",
      ":- Relation [_c0#4031,_c1#4032,_c2#4033,_c3#4034,_c4#4035,_c5#4036,_c6#4037,_c7#4038,_c8#4039,_c9#4040,_c10#4041,_c11#4042] ExcelRelation(com.crealytics.spark.excel.CellRangeAddressDataLocator@22770708,false,true,false,false,false,false,None,None,10,com.crealytics.spark.excel.StreamingWorkbookReader@1142aa3b)\n",
      ":- Relation [_c0#4067,_c1#4068,_c2#4069,_c3#4070,_c4#4071,_c5#4072,_c6#4073,_c7#4074,_c8#4075,_c9#4076,_c10#4077,_c11#4078] ExcelRelation(com.crealytics.spark.excel.CellRangeAddressDataLocator@715a43ea,false,true,false,false,false,false,None,None,10,com.crealytics.spark.excel.StreamingWorkbookReader@391849c2)\n",
      ":- Relation [_c0#4103,_c1#4104,_c2#4105,_c3#4106,_c4#4107,_c5#4108,_c6#4109,_c7#4110,_c8#4111,_c9#4112,_c10#4113,_c11#4114] ExcelRelation(com.crealytics.spark.excel.CellRangeAddressDataLocator@e4922ce,false,true,false,false,false,false,None,None,10,com.crealytics.spark.excel.StreamingWorkbookReader@49a5932c)\n",
      "+- Relation [_c0#4139,_c1#4140,_c2#4141,_c3#4142,_c4#4143,_c5#4144,_c6#4145,_c7#4146,_c8#4147,_c9#4148,_c10#4149,_c11#4150,_c12#4151] ExcelRelation(com.crealytics.spark.excel.CellRangeAddressDataLocator@77d21447,false,true,false,false,false,false,None,None,10,com.crealytics.spark.excel.StreamingWorkbookReader@7db55a06)\n",
      "\n",
      "Processing file 15/20: hdfs://localhost:9000/user/hadoop/inputs/NOVEMBER-2021.xlsx\n",
      "Successfully processed hdfs://localhost:9000/user/hadoop/inputs/NOVEMBER-2021.xlsx\n",
      "Processing file 16/20: hdfs://localhost:9000/user/hadoop/inputs/OCTOBER-2020.xlsx\n",
      "Error processing hdfs://localhost:9000/user/hadoop/inputs/OCTOBER-2020.xlsx: [NUM_COLUMNS_MISMATCH] UNION can only be performed on inputs with the same number of columns, but the first input has 12 columns and the 15th input has 13 columns.;\n",
      "'Union false, false\n",
      ":- Relation [_c0#3683,_c1#3684,_c2#3685,_c3#3686,_c4#3687,_c5#3688,_c6#3689,_c7#3690,_c8#3691,_c9#3692,_c10#3693,_c11#3694] ExcelRelation(com.crealytics.spark.excel.CellRangeAddressDataLocator@5daaceb9,false,true,false,false,false,false,None,None,10,com.crealytics.spark.excel.StreamingWorkbookReader@53675641)\n",
      ":- Relation [_c0#3707,_c1#3708,_c2#3709,_c3#3710,_c4#3711,_c5#3712,_c6#3713,_c7#3714,_c8#3715,_c9#3716,_c10#3717,_c11#3718] ExcelRelation(com.crealytics.spark.excel.CellRangeAddressDataLocator@417e0ef3,false,true,false,false,false,false,None,None,10,com.crealytics.spark.excel.StreamingWorkbookReader@3ab5d399)\n",
      ":- Relation [_c0#3743,_c1#3744,_c2#3745,_c3#3746,_c4#3747,_c5#3748,_c6#3749,_c7#3750,_c8#3751,_c9#3752,_c10#3753,_c11#3754] ExcelRelation(com.crealytics.spark.excel.CellRangeAddressDataLocator@31e10d70,false,true,false,false,false,false,None,None,10,com.crealytics.spark.excel.StreamingWorkbookReader@585d9053)\n",
      ":- Relation [_c0#3779,_c1#3780,_c2#3781,_c3#3782,_c4#3783,_c5#3784,_c6#3785,_c7#3786,_c8#3787,_c9#3788,_c10#3789,_c11#3790] ExcelRelation(com.crealytics.spark.excel.CellRangeAddressDataLocator@69e5143d,false,true,false,false,false,false,None,None,10,com.crealytics.spark.excel.StreamingWorkbookReader@7d582b83)\n",
      ":- Relation [_c0#3815,_c1#3816,_c2#3817,_c3#3818,_c4#3819,_c5#3820,_c6#3821,_c7#3822,_c8#3823,_c9#3824,_c10#3825,_c11#3826] ExcelRelation(com.crealytics.spark.excel.CellRangeAddressDataLocator@446b33d5,false,true,false,false,false,false,None,None,10,com.crealytics.spark.excel.StreamingWorkbookReader@7080ec03)\n",
      ":- Relation [_c0#3851,_c1#3852,_c2#3853,_c3#3854,_c4#3855,_c5#3856,_c6#3857,_c7#3858,_c8#3859,_c9#3860,_c10#3861,_c11#3862] ExcelRelation(com.crealytics.spark.excel.CellRangeAddressDataLocator@d2c4c47,false,true,false,false,false,false,None,None,10,com.crealytics.spark.excel.StreamingWorkbookReader@37ec87b6)\n",
      ":- Relation [_c0#3887,_c1#3888,_c2#3889,_c3#3890,_c4#3891,_c5#3892,_c6#3893,_c7#3894,_c8#3895,_c9#3896,_c10#3897,_c11#3898] ExcelRelation(com.crealytics.spark.excel.CellRangeAddressDataLocator@3fd56dea,false,true,false,false,false,false,None,None,10,com.crealytics.spark.excel.StreamingWorkbookReader@6fd10ecd)\n",
      ":- Relation [_c0#3923,_c1#3924,_c2#3925,_c3#3926,_c4#3927,_c5#3928,_c6#3929,_c7#3930,_c8#3931,_c9#3932,_c10#3933,_c11#3934] ExcelRelation(com.crealytics.spark.excel.CellRangeAddressDataLocator@47809630,false,true,false,false,false,false,None,None,10,com.crealytics.spark.excel.StreamingWorkbookReader@3271f6d)\n",
      ":- Relation [_c0#3959,_c1#3960,_c2#3961,_c3#3962,_c4#3963,_c5#3964,_c6#3965,_c7#3966,_c8#3967,_c9#3968,_c10#3969,_c11#3970] ExcelRelation(com.crealytics.spark.excel.CellRangeAddressDataLocator@31769d1a,false,true,false,false,false,false,None,None,10,com.crealytics.spark.excel.StreamingWorkbookReader@4450b4c9)\n",
      ":- Relation [_c0#3995,_c1#3996,_c2#3997,_c3#3998,_c4#3999,_c5#4000,_c6#4001,_c7#4002,_c8#4003,_c9#4004,_c10#4005,_c11#4006] ExcelRelation(com.crealytics.spark.excel.CellRangeAddressDataLocator@2a7f4349,false,true,false,false,false,false,None,None,10,com.crealytics.spark.excel.StreamingWorkbookReader@5d86dc0d)\n",
      ":- Relation [_c0#4031,_c1#4032,_c2#4033,_c3#4034,_c4#4035,_c5#4036,_c6#4037,_c7#4038,_c8#4039,_c9#4040,_c10#4041,_c11#4042] ExcelRelation(com.crealytics.spark.excel.CellRangeAddressDataLocator@22770708,false,true,false,false,false,false,None,None,10,com.crealytics.spark.excel.StreamingWorkbookReader@1142aa3b)\n",
      ":- Relation [_c0#4067,_c1#4068,_c2#4069,_c3#4070,_c4#4071,_c5#4072,_c6#4073,_c7#4074,_c8#4075,_c9#4076,_c10#4077,_c11#4078] ExcelRelation(com.crealytics.spark.excel.CellRangeAddressDataLocator@715a43ea,false,true,false,false,false,false,None,None,10,com.crealytics.spark.excel.StreamingWorkbookReader@391849c2)\n",
      ":- Relation [_c0#4103,_c1#4104,_c2#4105,_c3#4106,_c4#4107,_c5#4108,_c6#4109,_c7#4110,_c8#4111,_c9#4112,_c10#4113,_c11#4114] ExcelRelation(com.crealytics.spark.excel.CellRangeAddressDataLocator@e4922ce,false,true,false,false,false,false,None,None,10,com.crealytics.spark.excel.StreamingWorkbookReader@49a5932c)\n",
      ":- Relation [_c0#4165,_c1#4166,_c2#4167,_c3#4168,_c4#4169,_c5#4170,_c6#4171,_c7#4172,_c8#4173,_c9#4174,_c10#4175,_c11#4176] ExcelRelation(com.crealytics.spark.excel.CellRangeAddressDataLocator@6900460c,false,true,false,false,false,false,None,None,10,com.crealytics.spark.excel.StreamingWorkbookReader@5957204d)\n",
      "+- Relation [_c0#4201,_c1#4202,_c2#4203,_c3#4204,_c4#4205,_c5#4206,_c6#4207,_c7#4208,_c8#4209,_c9#4210,_c10#4211,_c11#4212,_c12#4213] ExcelRelation(com.crealytics.spark.excel.CellRangeAddressDataLocator@7586e202,false,true,false,false,false,false,None,None,10,com.crealytics.spark.excel.StreamingWorkbookReader@650c8530)\n",
      "\n",
      "Processing file 17/20: hdfs://localhost:9000/user/hadoop/inputs/OCTOBER-2021.xlsx\n",
      "Successfully processed hdfs://localhost:9000/user/hadoop/inputs/OCTOBER-2021.xlsx\n",
      "Processing file 18/20: hdfs://localhost:9000/user/hadoop/inputs/SEPTEMBER-2020.xlsx\n",
      "Error processing hdfs://localhost:9000/user/hadoop/inputs/SEPTEMBER-2020.xlsx: [NUM_COLUMNS_MISMATCH] UNION can only be performed on inputs with the same number of columns, but the first input has 12 columns and the 16th input has 13 columns.;\n",
      "'Union false, false\n",
      ":- Relation [_c0#3683,_c1#3684,_c2#3685,_c3#3686,_c4#3687,_c5#3688,_c6#3689,_c7#3690,_c8#3691,_c9#3692,_c10#3693,_c11#3694] ExcelRelation(com.crealytics.spark.excel.CellRangeAddressDataLocator@5daaceb9,false,true,false,false,false,false,None,None,10,com.crealytics.spark.excel.StreamingWorkbookReader@53675641)\n",
      ":- Relation [_c0#3707,_c1#3708,_c2#3709,_c3#3710,_c4#3711,_c5#3712,_c6#3713,_c7#3714,_c8#3715,_c9#3716,_c10#3717,_c11#3718] ExcelRelation(com.crealytics.spark.excel.CellRangeAddressDataLocator@417e0ef3,false,true,false,false,false,false,None,None,10,com.crealytics.spark.excel.StreamingWorkbookReader@3ab5d399)\n",
      ":- Relation [_c0#3743,_c1#3744,_c2#3745,_c3#3746,_c4#3747,_c5#3748,_c6#3749,_c7#3750,_c8#3751,_c9#3752,_c10#3753,_c11#3754] ExcelRelation(com.crealytics.spark.excel.CellRangeAddressDataLocator@31e10d70,false,true,false,false,false,false,None,None,10,com.crealytics.spark.excel.StreamingWorkbookReader@585d9053)\n",
      ":- Relation [_c0#3779,_c1#3780,_c2#3781,_c3#3782,_c4#3783,_c5#3784,_c6#3785,_c7#3786,_c8#3787,_c9#3788,_c10#3789,_c11#3790] ExcelRelation(com.crealytics.spark.excel.CellRangeAddressDataLocator@69e5143d,false,true,false,false,false,false,None,None,10,com.crealytics.spark.excel.StreamingWorkbookReader@7d582b83)\n",
      ":- Relation [_c0#3815,_c1#3816,_c2#3817,_c3#3818,_c4#3819,_c5#3820,_c6#3821,_c7#3822,_c8#3823,_c9#3824,_c10#3825,_c11#3826] ExcelRelation(com.crealytics.spark.excel.CellRangeAddressDataLocator@446b33d5,false,true,false,false,false,false,None,None,10,com.crealytics.spark.excel.StreamingWorkbookReader@7080ec03)\n",
      ":- Relation [_c0#3851,_c1#3852,_c2#3853,_c3#3854,_c4#3855,_c5#3856,_c6#3857,_c7#3858,_c8#3859,_c9#3860,_c10#3861,_c11#3862] ExcelRelation(com.crealytics.spark.excel.CellRangeAddressDataLocator@d2c4c47,false,true,false,false,false,false,None,None,10,com.crealytics.spark.excel.StreamingWorkbookReader@37ec87b6)\n",
      ":- Relation [_c0#3887,_c1#3888,_c2#3889,_c3#3890,_c4#3891,_c5#3892,_c6#3893,_c7#3894,_c8#3895,_c9#3896,_c10#3897,_c11#3898] ExcelRelation(com.crealytics.spark.excel.CellRangeAddressDataLocator@3fd56dea,false,true,false,false,false,false,None,None,10,com.crealytics.spark.excel.StreamingWorkbookReader@6fd10ecd)\n",
      ":- Relation [_c0#3923,_c1#3924,_c2#3925,_c3#3926,_c4#3927,_c5#3928,_c6#3929,_c7#3930,_c8#3931,_c9#3932,_c10#3933,_c11#3934] ExcelRelation(com.crealytics.spark.excel.CellRangeAddressDataLocator@47809630,false,true,false,false,false,false,None,None,10,com.crealytics.spark.excel.StreamingWorkbookReader@3271f6d)\n",
      ":- Relation [_c0#3959,_c1#3960,_c2#3961,_c3#3962,_c4#3963,_c5#3964,_c6#3965,_c7#3966,_c8#3967,_c9#3968,_c10#3969,_c11#3970] ExcelRelation(com.crealytics.spark.excel.CellRangeAddressDataLocator@31769d1a,false,true,false,false,false,false,None,None,10,com.crealytics.spark.excel.StreamingWorkbookReader@4450b4c9)\n",
      ":- Relation [_c0#3995,_c1#3996,_c2#3997,_c3#3998,_c4#3999,_c5#4000,_c6#4001,_c7#4002,_c8#4003,_c9#4004,_c10#4005,_c11#4006] ExcelRelation(com.crealytics.spark.excel.CellRangeAddressDataLocator@2a7f4349,false,true,false,false,false,false,None,None,10,com.crealytics.spark.excel.StreamingWorkbookReader@5d86dc0d)\n",
      ":- Relation [_c0#4031,_c1#4032,_c2#4033,_c3#4034,_c4#4035,_c5#4036,_c6#4037,_c7#4038,_c8#4039,_c9#4040,_c10#4041,_c11#4042] ExcelRelation(com.crealytics.spark.excel.CellRangeAddressDataLocator@22770708,false,true,false,false,false,false,None,None,10,com.crealytics.spark.excel.StreamingWorkbookReader@1142aa3b)\n",
      ":- Relation [_c0#4067,_c1#4068,_c2#4069,_c3#4070,_c4#4071,_c5#4072,_c6#4073,_c7#4074,_c8#4075,_c9#4076,_c10#4077,_c11#4078] ExcelRelation(com.crealytics.spark.excel.CellRangeAddressDataLocator@715a43ea,false,true,false,false,false,false,None,None,10,com.crealytics.spark.excel.StreamingWorkbookReader@391849c2)\n",
      ":- Relation [_c0#4103,_c1#4104,_c2#4105,_c3#4106,_c4#4107,_c5#4108,_c6#4109,_c7#4110,_c8#4111,_c9#4112,_c10#4113,_c11#4114] ExcelRelation(com.crealytics.spark.excel.CellRangeAddressDataLocator@e4922ce,false,true,false,false,false,false,None,None,10,com.crealytics.spark.excel.StreamingWorkbookReader@49a5932c)\n",
      ":- Relation [_c0#4165,_c1#4166,_c2#4167,_c3#4168,_c4#4169,_c5#4170,_c6#4171,_c7#4172,_c8#4173,_c9#4174,_c10#4175,_c11#4176] ExcelRelation(com.crealytics.spark.excel.CellRangeAddressDataLocator@6900460c,false,true,false,false,false,false,None,None,10,com.crealytics.spark.excel.StreamingWorkbookReader@5957204d)\n",
      ":- Relation [_c0#4227,_c1#4228,_c2#4229,_c3#4230,_c4#4231,_c5#4232,_c6#4233,_c7#4234,_c8#4235,_c9#4236,_c10#4237,_c11#4238] ExcelRelation(com.crealytics.spark.excel.CellRangeAddressDataLocator@50157e85,false,true,false,false,false,false,None,None,10,com.crealytics.spark.excel.StreamingWorkbookReader@51e9c8a1)\n",
      "+- Relation [_c0#4263,_c1#4264,_c2#4265,_c3#4266,_c4#4267,_c5#4268,_c6#4269,_c7#4270,_c8#4271,_c9#4272,_c10#4273,_c11#4274,_c12#4275] ExcelRelation(com.crealytics.spark.excel.CellRangeAddressDataLocator@f2c89f2,false,true,false,false,false,false,None,None,10,com.crealytics.spark.excel.StreamingWorkbookReader@2c032624)\n",
      "\n",
      "Processing file 19/20: hdfs://localhost:9000/user/hadoop/inputs/SEPTEMBER-2021.xlsx\n",
      "Successfully processed hdfs://localhost:9000/user/hadoop/inputs/SEPTEMBER-2021.xlsx\n",
      "Processing file 20/20: hdfs://localhost:9000/user/hadoop/inputs/jUNE-2021.xlsx\n",
      "Successfully processed hdfs://localhost:9000/user/hadoop/inputs/jUNE-2021.xlsx\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data combination completed successfully\n"
     ]
    }
   ],
   "source": [
    "# Stop any existing SparkSession\n",
    "if 'spark' in locals():\n",
    "    spark.stop()\n",
    "    \n",
    "# Create new SparkSession with minimal memory settings\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"Weather Data Combination\") \\\n",
    "    .config(\"spark.jars.packages\", \"com.crealytics:spark-excel_2.12:0.13.7\") \\\n",
    "    .config(\"spark.driver.memory\", \"2g\") \\\n",
    "    .config(\"spark.executor.memory\", \"2g\") \\\n",
    "    .config(\"spark.sql.shuffle.partitions\", \"2\") \\\n",
    "    .config(\"spark.driver.maxResultSize\", \"1g\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "\n",
    "def process_and_combine_files(file_paths, output_path):\n",
    "    \"\"\"\n",
    "    Process multiple Excel files and combine them into a single CSV file\n",
    "    while ensuring all columns are present and properly typed.\n",
    "    \"\"\"\n",
    "    combined_df = None\n",
    "    \n",
    "    for i, file_path in enumerate(file_paths):\n",
    "        try:\n",
    "            print(f\"Processing file {i+1}/{len(file_paths)}: {file_path}\")\n",
    "            \n",
    "            # Read Excel file\n",
    "            current_df = spark.read.format(\"com.crealytics.spark.excel\") \\\n",
    "                .option(\"header\", \"false\") \\\n",
    "                .option(\"dataAddress\", \"'RUA Data'!A6\") \\\n",
    "                .option(\"maxRowsInMemory\", 1000) \\\n",
    "                .option(\"treatEmptyValuesAsNulls\", \"true\") \\\n",
    "                .load(file_path)\n",
    "            \n",
    "            # Select columns in specific order to ensure consistency\n",
    "            \n",
    "            # Combine DataFrames\n",
    "            if combined_df is None:\n",
    "                combined_df = current_df\n",
    "            else:\n",
    "                combined_df = combined_df.union(current_df)\n",
    "            \n",
    "            # Clear cache after each file\n",
    "            current_df.unpersist()\n",
    "            spark.catalog.clearCache()\n",
    "            \n",
    "            print(f\"Successfully processed {file_path}\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error processing {file_path}: {str(e)}\")\n",
    "            continue\n",
    "    \n",
    "    # Write combined data to CSV\n",
    "    if combined_df is not None:\n",
    "        combined_df.write \\\n",
    "            .mode(\"overwrite\") \\\n",
    "            .option(\"header\", \"true\") \\\n",
    "            .option(\"compression\", \"none\") \\\n",
    "            .csv(output_path)\n",
    "    else:\n",
    "        print(\"No data was successfully processed\")\n",
    "\n",
    "# Execute the combination process\n",
    "try:\n",
    "    process_and_combine_files(file_paths, f'{MERGED_PATH}/combined_raw_data.csv')\n",
    "    print(\"Data combination completed successfully\")\n",
    "except Exception as e:\n",
    "    print(f\"Error in main process: {str(e)}\")\n",
    "finally:\n",
    "    # Clean up\n",
    "    spark.catalog.clearCache()\n",
    "    spark.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Verify the file write successfully"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- _c0: string (nullable = true)\n",
      " |-- _c1: string (nullable = true)\n",
      " |-- _c2: string (nullable = true)\n",
      " |-- _c3: string (nullable = true)\n",
      " |-- _c4: string (nullable = true)\n",
      " |-- _c5: string (nullable = true)\n",
      " |-- _c6: string (nullable = true)\n",
      " |-- _c7: string (nullable = true)\n",
      " |-- _c8: string (nullable = true)\n",
      " |-- _c9: string (nullable = true)\n",
      " |-- _c10: string (nullable = true)\n",
      " |-- _c11: string (nullable = true)\n",
      "\n",
      "+----+--------+--------+------+---+---+-----+----+---+---+----+-----+\n",
      "| _c0|     _c1|     _c2|   _c3|_c4|_c5|  _c6| _c7|_c8|_c9|_c10| _c11|\n",
      "+----+--------+--------+------+---+---+-----+----+---+---+----+-----+\n",
      "|3601|21/05/13|12:00:00| 0.278|233|  0|34.92|60.8|0.3|1.7| 125|26.24|\n",
      "|3602|21/05/13|12:05:00|0.2776|751|  0|34.26|60.9|0.3|1.3| 124|25.65|\n",
      "|3603|21/05/13|12:10:00|0.2776|963|  0|35.18|59.6|0.3|1.3| 114|26.15|\n",
      "|3604|21/05/13|12:15:00|0.2776|956|  0| 35.8|57.5|0.3|1.3| 107|26.12|\n",
      "|3605|21/05/13|12:20:00|0.2776|994|  0|35.98|58.8|0.3|1.3| 105|26.68|\n",
      "+----+--------+--------+------+---+---+-----+----+---+---+----+-----+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# Initialize Spark session\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"Read CSV from HDFS\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# Path to the CSV file on HDFS\n",
    "file_path = f'{MERGED_PATH}/combined_raw_data.csv'\n",
    "\n",
    "# Read the CSV file into a DataFrame\n",
    "df = spark.read.csv(file_path, header=True)\n",
    "\n",
    "df.printSchema()\n",
    "df.show(5)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
