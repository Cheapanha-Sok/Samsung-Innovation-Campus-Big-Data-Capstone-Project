{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Wrangling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Import necessary modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import StructType, StructField, StringType, FloatType\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Configure the hadoop dir path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input path hdfs://localhost:9000/user/hadoop/inputs , merge path hdfs://localhost:9000/user/hadoop/merged\n"
     ]
    }
   ],
   "source": [
    "BASE_PATH = 'hdfs://localhost:9000/user/hadoop'\n",
    "INPUT_PATH = f'{BASE_PATH}/inputs'\n",
    "MERGED_PATH = f'{BASE_PATH}/merged'\n",
    "\n",
    "print('input path {} , merge path {}'.format(INPUT_PATH, MERGED_PATH))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Function for create spark session that connect to hadoop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_spark_session():\n",
    "    \"\"\"\n",
    "    Creates and configures a SparkSession with minimal memory settings.\n",
    "    \"\"\"\n",
    "    spark = SparkSession.builder \\\n",
    "    .appName(\"Weather Data Combination\") \\\n",
    "    .config(\"spark.jars.packages\", \"com.crealytics:spark-excel_2.12:0.13.7\") \\\n",
    "    .config(\"spark.driver.memory\", \"2g\") \\\n",
    "    .config(\"spark.executor.memory\", \"2g\") \\\n",
    "    .config(\"spark.driver.maxResultSize\", \"1g\") \\\n",
    "    .config(\"spark.ui.showConsoleProgress\", \"true\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "    return spark\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Create a function to load data from hadoop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_excel_files(spark, file_path):\n",
    "    \"\"\"\n",
    "    Load all Excel files from the given base path.\n",
    "    \"\"\"\n",
    "    df = spark.read.format(\"com.crealytics.spark.excel\") \\\n",
    "                .option(\"header\", \"false\") \\\n",
    "                .option(\"dataAddress\", \"'RUA Data'!A6\") \\\n",
    "                .option(\"maxRowsInMemory\", 1000) \\\n",
    "                .option(\"treatEmptyValuesAsNulls\", \"true\") \\\n",
    "                .option(\"inferSchema\", \"false\") \\\n",
    "                .load(file_path)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Test read data as xlsx file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- _c0: string (nullable = true)\n",
      " |-- _c1: string (nullable = true)\n",
      " |-- _c2: string (nullable = true)\n",
      " |-- _c3: string (nullable = true)\n",
      " |-- _c4: string (nullable = true)\n",
      " |-- _c5: string (nullable = true)\n",
      " |-- _c6: string (nullable = true)\n",
      " |-- _c7: string (nullable = true)\n",
      " |-- _c8: string (nullable = true)\n",
      " |-- _c9: string (nullable = true)\n",
      " |-- _c10: string (nullable = true)\n",
      " |-- _c11: string (nullable = true)\n",
      "\n",
      "+---+--------+--------+------+---+---+-----+----+---+---+----+-----+\n",
      "|_c0|     _c1|     _c2|   _c3|_c4|_c5|  _c6| _c7|_c8|_c9|_c10| _c11|\n",
      "+---+--------+--------+------+---+---+-----+----+---+---+----+-----+\n",
      "|  1|21/04/01|00:00:00|0.2534|  1|  0|28.12|80.6|  0|0.3| 149| 24.5|\n",
      "|  2|21/04/01|00:05:00|0.2532|  1|  0|28.02|  81|  0|  0| 215|24.49|\n",
      "|  3|21/04/01|00:10:00|0.2524|  1|  0|28.07|  81|  0|1.3| 170|24.53|\n",
      "|  4|21/04/01|00:15:00|0.2524|  1|  0| 28.1|80.8|  0|1.7| 166|24.52|\n",
      "|  5|21/04/01|00:20:00|0.2524|  1|  0|28.07|80.8|0.3|2.7| 181|24.49|\n",
      "+---+--------+--------+------+---+---+-----+----+---+---+----+-----+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark = create_spark_session()\n",
    "hdfs_path = f'{INPUT_PATH}/APRIL-2021.xlsx'\n",
    "df = load_excel_files(spark, hdfs_path)\n",
    "df.printSchema()\n",
    "df.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Configure the files that we need to work with"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['hdfs://localhost:9000/user/hadoop/inputs/APRIL-2021.xlsx', 'hdfs://localhost:9000/user/hadoop/inputs/APRIL-2022.xlsx', 'hdfs://localhost:9000/user/hadoop/inputs/AUGUST-2021.xlsx', 'hdfs://localhost:9000/user/hadoop/inputs/DECEMBER-2020.xlsx', 'hdfs://localhost:9000/user/hadoop/inputs/DECEMBER-2021.xlsx', 'hdfs://localhost:9000/user/hadoop/inputs/FEBRUARY-2021.xlsx', 'hdfs://localhost:9000/user/hadoop/inputs/FEBRUARY-2022.xlsx', 'hdfs://localhost:9000/user/hadoop/inputs/JANUARY-2021.xlsx', 'hdfs://localhost:9000/user/hadoop/inputs/JANUARY-2022.xlsx', 'hdfs://localhost:9000/user/hadoop/inputs/JULY-2021.xlsx', 'hdfs://localhost:9000/user/hadoop/inputs/MARCH-2021.xlsx', 'hdfs://localhost:9000/user/hadoop/inputs/MARCH-2022.xlsx', 'hdfs://localhost:9000/user/hadoop/inputs/MAY-2021.xlsx', 'hdfs://localhost:9000/user/hadoop/inputs/NOVEMBER-2020.xlsx', 'hdfs://localhost:9000/user/hadoop/inputs/NOVEMBER-2021.xlsx', 'hdfs://localhost:9000/user/hadoop/inputs/OCTOBER-2020.xlsx', 'hdfs://localhost:9000/user/hadoop/inputs/OCTOBER-2021.xlsx', 'hdfs://localhost:9000/user/hadoop/inputs/SEPTEMBER-2020.xlsx', 'hdfs://localhost:9000/user/hadoop/inputs/SEPTEMBER-2021.xlsx', 'hdfs://localhost:9000/user/hadoop/inputs/jUNE-2021.xlsx']\n"
     ]
    }
   ],
   "source": [
    "file_paths = [f\"{INPUT_PATH}/{filename}\" for filename in [\n",
    "    \"APRIL-2021.xlsx\", \"APRIL-2022.xlsx\", \"AUGUST-2021.xlsx\",\n",
    "    \"DECEMBER-2020.xlsx\", \"DECEMBER-2021.xlsx\", \"FEBRUARY-2021.xlsx\",\n",
    "    \"FEBRUARY-2022.xlsx\", \"JANUARY-2021.xlsx\", \"JANUARY-2022.xlsx\",\n",
    "    \"JULY-2021.xlsx\", \"MARCH-2021.xlsx\", \"MARCH-2022.xlsx\",\n",
    "    \"MAY-2021.xlsx\", \"NOVEMBER-2020.xlsx\", \"NOVEMBER-2021.xlsx\",\n",
    "    \"OCTOBER-2020.xlsx\", \"OCTOBER-2021.xlsx\", \"SEPTEMBER-2020.xlsx\",\n",
    "    \"SEPTEMBER-2021.xlsx\", \"jUNE-2021.xlsx\"\n",
    "]]\n",
    "print(file_paths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing file 1/20: hdfs://localhost:9000/user/hadoop/inputs/APRIL-2021.xlsx\n",
      "Successfully processed hdfs://localhost:9000/user/hadoop/inputs/APRIL-2021.xlsx\n",
      "Processing file 2/20: hdfs://localhost:9000/user/hadoop/inputs/APRIL-2022.xlsx\n",
      "Successfully processed hdfs://localhost:9000/user/hadoop/inputs/APRIL-2022.xlsx\n",
      "Processing file 3/20: hdfs://localhost:9000/user/hadoop/inputs/AUGUST-2021.xlsx\n",
      "Successfully processed hdfs://localhost:9000/user/hadoop/inputs/AUGUST-2021.xlsx\n",
      "Processing file 4/20: hdfs://localhost:9000/user/hadoop/inputs/DECEMBER-2020.xlsx\n",
      "Successfully processed hdfs://localhost:9000/user/hadoop/inputs/DECEMBER-2020.xlsx\n",
      "Processing file 5/20: hdfs://localhost:9000/user/hadoop/inputs/DECEMBER-2021.xlsx\n",
      "Successfully processed hdfs://localhost:9000/user/hadoop/inputs/DECEMBER-2021.xlsx\n",
      "Processing file 6/20: hdfs://localhost:9000/user/hadoop/inputs/FEBRUARY-2021.xlsx\n",
      "Successfully processed hdfs://localhost:9000/user/hadoop/inputs/FEBRUARY-2021.xlsx\n",
      "Processing file 7/20: hdfs://localhost:9000/user/hadoop/inputs/FEBRUARY-2022.xlsx\n",
      "Successfully processed hdfs://localhost:9000/user/hadoop/inputs/FEBRUARY-2022.xlsx\n",
      "Processing file 8/20: hdfs://localhost:9000/user/hadoop/inputs/JANUARY-2021.xlsx\n",
      "Successfully processed hdfs://localhost:9000/user/hadoop/inputs/JANUARY-2021.xlsx\n",
      "Processing file 9/20: hdfs://localhost:9000/user/hadoop/inputs/JANUARY-2022.xlsx\n",
      "Successfully processed hdfs://localhost:9000/user/hadoop/inputs/JANUARY-2022.xlsx\n",
      "Processing file 10/20: hdfs://localhost:9000/user/hadoop/inputs/JULY-2021.xlsx\n",
      "Successfully processed hdfs://localhost:9000/user/hadoop/inputs/JULY-2021.xlsx\n",
      "Processing file 11/20: hdfs://localhost:9000/user/hadoop/inputs/MARCH-2021.xlsx\n",
      "Successfully processed hdfs://localhost:9000/user/hadoop/inputs/MARCH-2021.xlsx\n",
      "Processing file 12/20: hdfs://localhost:9000/user/hadoop/inputs/MARCH-2022.xlsx\n",
      "Successfully processed hdfs://localhost:9000/user/hadoop/inputs/MARCH-2022.xlsx\n",
      "Processing file 13/20: hdfs://localhost:9000/user/hadoop/inputs/MAY-2021.xlsx\n",
      "Successfully processed hdfs://localhost:9000/user/hadoop/inputs/MAY-2021.xlsx\n",
      "Processing file 14/20: hdfs://localhost:9000/user/hadoop/inputs/NOVEMBER-2020.xlsx\n",
      "Error processing hdfs://localhost:9000/user/hadoop/inputs/NOVEMBER-2020.xlsx: [NUM_COLUMNS_MISMATCH] UNION can only be performed on inputs with the same number of columns, but the first input has 12 columns and the 14th input has 13 columns.;\n",
      "'Union false, false\n",
      ":- Relation [_c0#170,_c1#171,_c2#172,_c3#173,_c4#174,_c5#175,_c6#176,_c7#177,_c8#178,_c9#179,_c10#180,_c11#181] ExcelRelation(com.crealytics.spark.excel.CellRangeAddressDataLocator@72f815a0,false,true,false,false,false,false,None,None,10,com.crealytics.spark.excel.StreamingWorkbookReader@71a4a358)\n",
      ":- Relation [_c0#194,_c1#195,_c2#196,_c3#197,_c4#198,_c5#199,_c6#200,_c7#201,_c8#202,_c9#203,_c10#204,_c11#205] ExcelRelation(com.crealytics.spark.excel.CellRangeAddressDataLocator@4b6ef163,false,true,false,false,false,false,None,None,10,com.crealytics.spark.excel.StreamingWorkbookReader@302ad1dc)\n",
      ":- Relation [_c0#230,_c1#231,_c2#232,_c3#233,_c4#234,_c5#235,_c6#236,_c7#237,_c8#238,_c9#239,_c10#240,_c11#241] ExcelRelation(com.crealytics.spark.excel.CellRangeAddressDataLocator@24d876c,false,true,false,false,false,false,None,None,10,com.crealytics.spark.excel.StreamingWorkbookReader@56a74a1e)\n",
      ":- Relation [_c0#266,_c1#267,_c2#268,_c3#269,_c4#270,_c5#271,_c6#272,_c7#273,_c8#274,_c9#275,_c10#276,_c11#277] ExcelRelation(com.crealytics.spark.excel.CellRangeAddressDataLocator@6a7e5c40,false,true,false,false,false,false,None,None,10,com.crealytics.spark.excel.StreamingWorkbookReader@c3cd4e2)\n",
      ":- Relation [_c0#302,_c1#303,_c2#304,_c3#305,_c4#306,_c5#307,_c6#308,_c7#309,_c8#310,_c9#311,_c10#312,_c11#313] ExcelRelation(com.crealytics.spark.excel.CellRangeAddressDataLocator@62a444ee,false,true,false,false,false,false,None,None,10,com.crealytics.spark.excel.StreamingWorkbookReader@ebf151e)\n",
      ":- Relation [_c0#338,_c1#339,_c2#340,_c3#341,_c4#342,_c5#343,_c6#344,_c7#345,_c8#346,_c9#347,_c10#348,_c11#349] ExcelRelation(com.crealytics.spark.excel.CellRangeAddressDataLocator@4d2cefc2,false,true,false,false,false,false,None,None,10,com.crealytics.spark.excel.StreamingWorkbookReader@2319f15d)\n",
      ":- Relation [_c0#374,_c1#375,_c2#376,_c3#377,_c4#378,_c5#379,_c6#380,_c7#381,_c8#382,_c9#383,_c10#384,_c11#385] ExcelRelation(com.crealytics.spark.excel.CellRangeAddressDataLocator@6f696fed,false,true,false,false,false,false,None,None,10,com.crealytics.spark.excel.StreamingWorkbookReader@405f9bc2)\n",
      ":- Relation [_c0#410,_c1#411,_c2#412,_c3#413,_c4#414,_c5#415,_c6#416,_c7#417,_c8#418,_c9#419,_c10#420,_c11#421] ExcelRelation(com.crealytics.spark.excel.CellRangeAddressDataLocator@7365a192,false,true,false,false,false,false,None,None,10,com.crealytics.spark.excel.StreamingWorkbookReader@78c35587)\n",
      ":- Relation [_c0#446,_c1#447,_c2#448,_c3#449,_c4#450,_c5#451,_c6#452,_c7#453,_c8#454,_c9#455,_c10#456,_c11#457] ExcelRelation(com.crealytics.spark.excel.CellRangeAddressDataLocator@5d5d0ea7,false,true,false,false,false,false,None,None,10,com.crealytics.spark.excel.StreamingWorkbookReader@382def25)\n",
      ":- Relation [_c0#482,_c1#483,_c2#484,_c3#485,_c4#486,_c5#487,_c6#488,_c7#489,_c8#490,_c9#491,_c10#492,_c11#493] ExcelRelation(com.crealytics.spark.excel.CellRangeAddressDataLocator@171a1aed,false,true,false,false,false,false,None,None,10,com.crealytics.spark.excel.StreamingWorkbookReader@54d219f)\n",
      ":- Relation [_c0#518,_c1#519,_c2#520,_c3#521,_c4#522,_c5#523,_c6#524,_c7#525,_c8#526,_c9#527,_c10#528,_c11#529] ExcelRelation(com.crealytics.spark.excel.CellRangeAddressDataLocator@2cf7d1b4,false,true,false,false,false,false,None,None,10,com.crealytics.spark.excel.StreamingWorkbookReader@456e9f1f)\n",
      ":- Relation [_c0#554,_c1#555,_c2#556,_c3#557,_c4#558,_c5#559,_c6#560,_c7#561,_c8#562,_c9#563,_c10#564,_c11#565] ExcelRelation(com.crealytics.spark.excel.CellRangeAddressDataLocator@4878ef50,false,true,false,false,false,false,None,None,10,com.crealytics.spark.excel.StreamingWorkbookReader@312b9226)\n",
      ":- Relation [_c0#590,_c1#591,_c2#592,_c3#593,_c4#594,_c5#595,_c6#596,_c7#597,_c8#598,_c9#599,_c10#600,_c11#601] ExcelRelation(com.crealytics.spark.excel.CellRangeAddressDataLocator@7b8bd4bd,false,true,false,false,false,false,None,None,10,com.crealytics.spark.excel.StreamingWorkbookReader@3cd5b649)\n",
      "+- Relation [_c0#626,_c1#627,_c2#628,_c3#629,_c4#630,_c5#631,_c6#632,_c7#633,_c8#634,_c9#635,_c10#636,_c11#637,_c12#638] ExcelRelation(com.crealytics.spark.excel.CellRangeAddressDataLocator@5f20add3,false,true,false,false,false,false,None,None,10,com.crealytics.spark.excel.StreamingWorkbookReader@516fc7ed)\n",
      "\n",
      "Processing file 15/20: hdfs://localhost:9000/user/hadoop/inputs/NOVEMBER-2021.xlsx\n",
      "Successfully processed hdfs://localhost:9000/user/hadoop/inputs/NOVEMBER-2021.xlsx\n",
      "Processing file 16/20: hdfs://localhost:9000/user/hadoop/inputs/OCTOBER-2020.xlsx\n",
      "Error processing hdfs://localhost:9000/user/hadoop/inputs/OCTOBER-2020.xlsx: [NUM_COLUMNS_MISMATCH] UNION can only be performed on inputs with the same number of columns, but the first input has 12 columns and the 15th input has 13 columns.;\n",
      "'Union false, false\n",
      ":- Relation [_c0#170,_c1#171,_c2#172,_c3#173,_c4#174,_c5#175,_c6#176,_c7#177,_c8#178,_c9#179,_c10#180,_c11#181] ExcelRelation(com.crealytics.spark.excel.CellRangeAddressDataLocator@72f815a0,false,true,false,false,false,false,None,None,10,com.crealytics.spark.excel.StreamingWorkbookReader@71a4a358)\n",
      ":- Relation [_c0#194,_c1#195,_c2#196,_c3#197,_c4#198,_c5#199,_c6#200,_c7#201,_c8#202,_c9#203,_c10#204,_c11#205] ExcelRelation(com.crealytics.spark.excel.CellRangeAddressDataLocator@4b6ef163,false,true,false,false,false,false,None,None,10,com.crealytics.spark.excel.StreamingWorkbookReader@302ad1dc)\n",
      ":- Relation [_c0#230,_c1#231,_c2#232,_c3#233,_c4#234,_c5#235,_c6#236,_c7#237,_c8#238,_c9#239,_c10#240,_c11#241] ExcelRelation(com.crealytics.spark.excel.CellRangeAddressDataLocator@24d876c,false,true,false,false,false,false,None,None,10,com.crealytics.spark.excel.StreamingWorkbookReader@56a74a1e)\n",
      ":- Relation [_c0#266,_c1#267,_c2#268,_c3#269,_c4#270,_c5#271,_c6#272,_c7#273,_c8#274,_c9#275,_c10#276,_c11#277] ExcelRelation(com.crealytics.spark.excel.CellRangeAddressDataLocator@6a7e5c40,false,true,false,false,false,false,None,None,10,com.crealytics.spark.excel.StreamingWorkbookReader@c3cd4e2)\n",
      ":- Relation [_c0#302,_c1#303,_c2#304,_c3#305,_c4#306,_c5#307,_c6#308,_c7#309,_c8#310,_c9#311,_c10#312,_c11#313] ExcelRelation(com.crealytics.spark.excel.CellRangeAddressDataLocator@62a444ee,false,true,false,false,false,false,None,None,10,com.crealytics.spark.excel.StreamingWorkbookReader@ebf151e)\n",
      ":- Relation [_c0#338,_c1#339,_c2#340,_c3#341,_c4#342,_c5#343,_c6#344,_c7#345,_c8#346,_c9#347,_c10#348,_c11#349] ExcelRelation(com.crealytics.spark.excel.CellRangeAddressDataLocator@4d2cefc2,false,true,false,false,false,false,None,None,10,com.crealytics.spark.excel.StreamingWorkbookReader@2319f15d)\n",
      ":- Relation [_c0#374,_c1#375,_c2#376,_c3#377,_c4#378,_c5#379,_c6#380,_c7#381,_c8#382,_c9#383,_c10#384,_c11#385] ExcelRelation(com.crealytics.spark.excel.CellRangeAddressDataLocator@6f696fed,false,true,false,false,false,false,None,None,10,com.crealytics.spark.excel.StreamingWorkbookReader@405f9bc2)\n",
      ":- Relation [_c0#410,_c1#411,_c2#412,_c3#413,_c4#414,_c5#415,_c6#416,_c7#417,_c8#418,_c9#419,_c10#420,_c11#421] ExcelRelation(com.crealytics.spark.excel.CellRangeAddressDataLocator@7365a192,false,true,false,false,false,false,None,None,10,com.crealytics.spark.excel.StreamingWorkbookReader@78c35587)\n",
      ":- Relation [_c0#446,_c1#447,_c2#448,_c3#449,_c4#450,_c5#451,_c6#452,_c7#453,_c8#454,_c9#455,_c10#456,_c11#457] ExcelRelation(com.crealytics.spark.excel.CellRangeAddressDataLocator@5d5d0ea7,false,true,false,false,false,false,None,None,10,com.crealytics.spark.excel.StreamingWorkbookReader@382def25)\n",
      ":- Relation [_c0#482,_c1#483,_c2#484,_c3#485,_c4#486,_c5#487,_c6#488,_c7#489,_c8#490,_c9#491,_c10#492,_c11#493] ExcelRelation(com.crealytics.spark.excel.CellRangeAddressDataLocator@171a1aed,false,true,false,false,false,false,None,None,10,com.crealytics.spark.excel.StreamingWorkbookReader@54d219f)\n",
      ":- Relation [_c0#518,_c1#519,_c2#520,_c3#521,_c4#522,_c5#523,_c6#524,_c7#525,_c8#526,_c9#527,_c10#528,_c11#529] ExcelRelation(com.crealytics.spark.excel.CellRangeAddressDataLocator@2cf7d1b4,false,true,false,false,false,false,None,None,10,com.crealytics.spark.excel.StreamingWorkbookReader@456e9f1f)\n",
      ":- Relation [_c0#554,_c1#555,_c2#556,_c3#557,_c4#558,_c5#559,_c6#560,_c7#561,_c8#562,_c9#563,_c10#564,_c11#565] ExcelRelation(com.crealytics.spark.excel.CellRangeAddressDataLocator@4878ef50,false,true,false,false,false,false,None,None,10,com.crealytics.spark.excel.StreamingWorkbookReader@312b9226)\n",
      ":- Relation [_c0#590,_c1#591,_c2#592,_c3#593,_c4#594,_c5#595,_c6#596,_c7#597,_c8#598,_c9#599,_c10#600,_c11#601] ExcelRelation(com.crealytics.spark.excel.CellRangeAddressDataLocator@7b8bd4bd,false,true,false,false,false,false,None,None,10,com.crealytics.spark.excel.StreamingWorkbookReader@3cd5b649)\n",
      ":- Relation [_c0#652,_c1#653,_c2#654,_c3#655,_c4#656,_c5#657,_c6#658,_c7#659,_c8#660,_c9#661,_c10#662,_c11#663] ExcelRelation(com.crealytics.spark.excel.CellRangeAddressDataLocator@33c3d13,false,true,false,false,false,false,None,None,10,com.crealytics.spark.excel.StreamingWorkbookReader@44462f1f)\n",
      "+- Relation [_c0#688,_c1#689,_c2#690,_c3#691,_c4#692,_c5#693,_c6#694,_c7#695,_c8#696,_c9#697,_c10#698,_c11#699,_c12#700] ExcelRelation(com.crealytics.spark.excel.CellRangeAddressDataLocator@3f10eb23,false,true,false,false,false,false,None,None,10,com.crealytics.spark.excel.StreamingWorkbookReader@4fbec00a)\n",
      "\n",
      "Processing file 17/20: hdfs://localhost:9000/user/hadoop/inputs/OCTOBER-2021.xlsx\n",
      "Successfully processed hdfs://localhost:9000/user/hadoop/inputs/OCTOBER-2021.xlsx\n",
      "Processing file 18/20: hdfs://localhost:9000/user/hadoop/inputs/SEPTEMBER-2020.xlsx\n",
      "Error processing hdfs://localhost:9000/user/hadoop/inputs/SEPTEMBER-2020.xlsx: [NUM_COLUMNS_MISMATCH] UNION can only be performed on inputs with the same number of columns, but the first input has 12 columns and the 16th input has 13 columns.;\n",
      "'Union false, false\n",
      ":- Relation [_c0#170,_c1#171,_c2#172,_c3#173,_c4#174,_c5#175,_c6#176,_c7#177,_c8#178,_c9#179,_c10#180,_c11#181] ExcelRelation(com.crealytics.spark.excel.CellRangeAddressDataLocator@72f815a0,false,true,false,false,false,false,None,None,10,com.crealytics.spark.excel.StreamingWorkbookReader@71a4a358)\n",
      ":- Relation [_c0#194,_c1#195,_c2#196,_c3#197,_c4#198,_c5#199,_c6#200,_c7#201,_c8#202,_c9#203,_c10#204,_c11#205] ExcelRelation(com.crealytics.spark.excel.CellRangeAddressDataLocator@4b6ef163,false,true,false,false,false,false,None,None,10,com.crealytics.spark.excel.StreamingWorkbookReader@302ad1dc)\n",
      ":- Relation [_c0#230,_c1#231,_c2#232,_c3#233,_c4#234,_c5#235,_c6#236,_c7#237,_c8#238,_c9#239,_c10#240,_c11#241] ExcelRelation(com.crealytics.spark.excel.CellRangeAddressDataLocator@24d876c,false,true,false,false,false,false,None,None,10,com.crealytics.spark.excel.StreamingWorkbookReader@56a74a1e)\n",
      ":- Relation [_c0#266,_c1#267,_c2#268,_c3#269,_c4#270,_c5#271,_c6#272,_c7#273,_c8#274,_c9#275,_c10#276,_c11#277] ExcelRelation(com.crealytics.spark.excel.CellRangeAddressDataLocator@6a7e5c40,false,true,false,false,false,false,None,None,10,com.crealytics.spark.excel.StreamingWorkbookReader@c3cd4e2)\n",
      ":- Relation [_c0#302,_c1#303,_c2#304,_c3#305,_c4#306,_c5#307,_c6#308,_c7#309,_c8#310,_c9#311,_c10#312,_c11#313] ExcelRelation(com.crealytics.spark.excel.CellRangeAddressDataLocator@62a444ee,false,true,false,false,false,false,None,None,10,com.crealytics.spark.excel.StreamingWorkbookReader@ebf151e)\n",
      ":- Relation [_c0#338,_c1#339,_c2#340,_c3#341,_c4#342,_c5#343,_c6#344,_c7#345,_c8#346,_c9#347,_c10#348,_c11#349] ExcelRelation(com.crealytics.spark.excel.CellRangeAddressDataLocator@4d2cefc2,false,true,false,false,false,false,None,None,10,com.crealytics.spark.excel.StreamingWorkbookReader@2319f15d)\n",
      ":- Relation [_c0#374,_c1#375,_c2#376,_c3#377,_c4#378,_c5#379,_c6#380,_c7#381,_c8#382,_c9#383,_c10#384,_c11#385] ExcelRelation(com.crealytics.spark.excel.CellRangeAddressDataLocator@6f696fed,false,true,false,false,false,false,None,None,10,com.crealytics.spark.excel.StreamingWorkbookReader@405f9bc2)\n",
      ":- Relation [_c0#410,_c1#411,_c2#412,_c3#413,_c4#414,_c5#415,_c6#416,_c7#417,_c8#418,_c9#419,_c10#420,_c11#421] ExcelRelation(com.crealytics.spark.excel.CellRangeAddressDataLocator@7365a192,false,true,false,false,false,false,None,None,10,com.crealytics.spark.excel.StreamingWorkbookReader@78c35587)\n",
      ":- Relation [_c0#446,_c1#447,_c2#448,_c3#449,_c4#450,_c5#451,_c6#452,_c7#453,_c8#454,_c9#455,_c10#456,_c11#457] ExcelRelation(com.crealytics.spark.excel.CellRangeAddressDataLocator@5d5d0ea7,false,true,false,false,false,false,None,None,10,com.crealytics.spark.excel.StreamingWorkbookReader@382def25)\n",
      ":- Relation [_c0#482,_c1#483,_c2#484,_c3#485,_c4#486,_c5#487,_c6#488,_c7#489,_c8#490,_c9#491,_c10#492,_c11#493] ExcelRelation(com.crealytics.spark.excel.CellRangeAddressDataLocator@171a1aed,false,true,false,false,false,false,None,None,10,com.crealytics.spark.excel.StreamingWorkbookReader@54d219f)\n",
      ":- Relation [_c0#518,_c1#519,_c2#520,_c3#521,_c4#522,_c5#523,_c6#524,_c7#525,_c8#526,_c9#527,_c10#528,_c11#529] ExcelRelation(com.crealytics.spark.excel.CellRangeAddressDataLocator@2cf7d1b4,false,true,false,false,false,false,None,None,10,com.crealytics.spark.excel.StreamingWorkbookReader@456e9f1f)\n",
      ":- Relation [_c0#554,_c1#555,_c2#556,_c3#557,_c4#558,_c5#559,_c6#560,_c7#561,_c8#562,_c9#563,_c10#564,_c11#565] ExcelRelation(com.crealytics.spark.excel.CellRangeAddressDataLocator@4878ef50,false,true,false,false,false,false,None,None,10,com.crealytics.spark.excel.StreamingWorkbookReader@312b9226)\n",
      ":- Relation [_c0#590,_c1#591,_c2#592,_c3#593,_c4#594,_c5#595,_c6#596,_c7#597,_c8#598,_c9#599,_c10#600,_c11#601] ExcelRelation(com.crealytics.spark.excel.CellRangeAddressDataLocator@7b8bd4bd,false,true,false,false,false,false,None,None,10,com.crealytics.spark.excel.StreamingWorkbookReader@3cd5b649)\n",
      ":- Relation [_c0#652,_c1#653,_c2#654,_c3#655,_c4#656,_c5#657,_c6#658,_c7#659,_c8#660,_c9#661,_c10#662,_c11#663] ExcelRelation(com.crealytics.spark.excel.CellRangeAddressDataLocator@33c3d13,false,true,false,false,false,false,None,None,10,com.crealytics.spark.excel.StreamingWorkbookReader@44462f1f)\n",
      ":- Relation [_c0#714,_c1#715,_c2#716,_c3#717,_c4#718,_c5#719,_c6#720,_c7#721,_c8#722,_c9#723,_c10#724,_c11#725] ExcelRelation(com.crealytics.spark.excel.CellRangeAddressDataLocator@26e05c73,false,true,false,false,false,false,None,None,10,com.crealytics.spark.excel.StreamingWorkbookReader@6b48b872)\n",
      "+- Relation [_c0#750,_c1#751,_c2#752,_c3#753,_c4#754,_c5#755,_c6#756,_c7#757,_c8#758,_c9#759,_c10#760,_c11#761,_c12#762] ExcelRelation(com.crealytics.spark.excel.CellRangeAddressDataLocator@5a51a751,false,true,false,false,false,false,None,None,10,com.crealytics.spark.excel.StreamingWorkbookReader@3be36e4e)\n",
      "\n",
      "Processing file 19/20: hdfs://localhost:9000/user/hadoop/inputs/SEPTEMBER-2021.xlsx\n",
      "Successfully processed hdfs://localhost:9000/user/hadoop/inputs/SEPTEMBER-2021.xlsx\n",
      "Processing file 20/20: hdfs://localhost:9000/user/hadoop/inputs/jUNE-2021.xlsx\n",
      "Successfully processed hdfs://localhost:9000/user/hadoop/inputs/jUNE-2021.xlsx\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data combination completed successfully\n"
     ]
    }
   ],
   "source": [
    "# Stop any existing SparkSession\n",
    "if 'spark' in locals():\n",
    "    spark.stop()\n",
    "    \n",
    "# Create new SparkSession with minimal memory settings\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"Weather Data Combination\") \\\n",
    "    .config(\"spark.jars.packages\", \"com.crealytics:spark-excel_2.12:0.13.7\") \\\n",
    "    .config(\"spark.driver.memory\", \"2g\") \\\n",
    "    .config(\"spark.executor.memory\", \"2g\") \\\n",
    "    .config(\"spark.sql.shuffle.partitions\", \"2\") \\\n",
    "    .config(\"spark.driver.maxResultSize\", \"1g\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "\n",
    "def process_and_combine_files(file_paths, output_path):\n",
    "    \"\"\"\n",
    "    Process multiple Excel files and combine them into a single CSV file\n",
    "    while ensuring all columns are present and properly typed.\n",
    "    \"\"\"\n",
    "    combined_df = None\n",
    "    \n",
    "    for i, file_path in enumerate(file_paths):\n",
    "        try:\n",
    "            print(f\"Processing file {i+1}/{len(file_paths)}: {file_path}\")\n",
    "            \n",
    "            # Read Excel file\n",
    "            current_df = spark.read.format(\"com.crealytics.spark.excel\") \\\n",
    "                .option(\"header\", \"false\") \\\n",
    "                .option(\"dataAddress\", \"'RUA Data'!A6\") \\\n",
    "                .option(\"maxRowsInMemory\", 1000) \\\n",
    "                .option(\"treatEmptyValuesAsNulls\", \"true\") \\\n",
    "                .load(file_path)\n",
    "            \n",
    "            # Select columns in specific order to ensure consistency\n",
    "            \n",
    "            # Combine DataFrames\n",
    "            if combined_df is None:\n",
    "                combined_df = current_df\n",
    "            else:\n",
    "                combined_df = combined_df.union(current_df)\n",
    "            \n",
    "            # Clear cache after each file\n",
    "            current_df.unpersist()\n",
    "            spark.catalog.clearCache()\n",
    "            \n",
    "            print(f\"Successfully processed {file_path}\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error processing {file_path}: {str(e)}\")\n",
    "            continue\n",
    "    \n",
    "    # Write combined data to CSV\n",
    "    if combined_df is not None:\n",
    "        combined_df.write \\\n",
    "            .mode(\"overwrite\") \\\n",
    "            .option(\"header\", \"true\") \\\n",
    "            .option(\"compression\", \"none\") \\\n",
    "            .csv(output_path)\n",
    "    else:\n",
    "        print(\"No data was successfully processed\")\n",
    "\n",
    "# Execute the combination process\n",
    "try:\n",
    "    process_and_combine_files(file_paths, f'{MERGED_PATH}/combined_raw_data.csv')\n",
    "    print(\"Data combination completed successfully\")\n",
    "except Exception as e:\n",
    "    print(f\"Error in main process: {str(e)}\")\n",
    "finally:\n",
    "    # Clean up\n",
    "    spark.catalog.clearCache()\n",
    "    spark.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Verify the file write successfully"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- _c0: string (nullable = true)\n",
      " |-- _c1: string (nullable = true)\n",
      " |-- _c2: string (nullable = true)\n",
      " |-- _c3: string (nullable = true)\n",
      " |-- _c4: string (nullable = true)\n",
      " |-- _c5: string (nullable = true)\n",
      " |-- _c6: string (nullable = true)\n",
      " |-- _c7: string (nullable = true)\n",
      " |-- _c8: string (nullable = true)\n",
      " |-- _c9: string (nullable = true)\n",
      " |-- _c10: string (nullable = true)\n",
      " |-- _c11: string (nullable = true)\n",
      "\n",
      "+----+--------+--------+------+---+---+-----+----+---+---+----+-----+\n",
      "| _c0|     _c1|     _c2|   _c3|_c4|_c5|  _c6| _c7|_c8|_c9|_c10| _c11|\n",
      "+----+--------+--------+------+---+---+-----+----+---+---+----+-----+\n",
      "|3601|21/05/13|12:00:00| 0.278|233|  0|34.92|60.8|0.3|1.7| 125|26.24|\n",
      "|3602|21/05/13|12:05:00|0.2776|751|  0|34.26|60.9|0.3|1.3| 124|25.65|\n",
      "|3603|21/05/13|12:10:00|0.2776|963|  0|35.18|59.6|0.3|1.3| 114|26.15|\n",
      "|3604|21/05/13|12:15:00|0.2776|956|  0| 35.8|57.5|0.3|1.3| 107|26.12|\n",
      "|3605|21/05/13|12:20:00|0.2776|994|  0|35.98|58.8|0.3|1.3| 105|26.68|\n",
      "+----+--------+--------+------+---+---+-----+----+---+---+----+-----+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# Initialize Spark session\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"Read CSV from HDFS\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# Path to the CSV file on HDFS\n",
    "file_path = f'{MERGED_PATH}/combined_raw_data.csv'\n",
    "\n",
    "# Read the CSV file into a DataFrame\n",
    "df = spark.read.csv(file_path, header=True)\n",
    "\n",
    "df.printSchema()\n",
    "df.show(5)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
