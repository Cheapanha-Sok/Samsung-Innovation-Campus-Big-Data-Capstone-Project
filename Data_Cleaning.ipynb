{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Cleaning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Import necessary modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import to_date, date_format"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Configure the hadoop dir path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input path hdfs://localhost:9000/user/hadoop/inputs , merge path hdfs://localhost:9000/user/hadoop/merged\n"
     ]
    }
   ],
   "source": [
    "BASE_PATH = 'hdfs://localhost:9000/user/hadoop'\n",
    "INPUT_PATH = f'{BASE_PATH}/inputs'\n",
    "MERGED_PATH = f'{BASE_PATH}/merged'\n",
    "CLEANED_PATH = f'{BASE_PATH}/cleaned'\n",
    "\n",
    "print('input path {} , merge path {}'.format(INPUT_PATH, MERGED_PATH))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Read the data that we have merged"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- _c0: string (nullable = true)\n",
      " |-- _c1: string (nullable = true)\n",
      " |-- _c2: string (nullable = true)\n",
      " |-- _c3: string (nullable = true)\n",
      " |-- _c4: string (nullable = true)\n",
      " |-- _c5: string (nullable = true)\n",
      " |-- _c6: string (nullable = true)\n",
      " |-- _c7: string (nullable = true)\n",
      " |-- _c8: string (nullable = true)\n",
      " |-- _c9: string (nullable = true)\n",
      " |-- _c10: string (nullable = true)\n",
      " |-- _c11: string (nullable = true)\n",
      "\n",
      "+----+--------+--------+------+---+---+-----+----+---+---+----+-----+\n",
      "| _c0|     _c1|     _c2|   _c3|_c4|_c5|  _c6| _c7|_c8|_c9|_c10| _c11|\n",
      "+----+--------+--------+------+---+---+-----+----+---+---+----+-----+\n",
      "|3601|21/05/13|12:00:00| 0.278|233|  0|34.92|60.8|0.3|1.7| 125|26.24|\n",
      "|3602|21/05/13|12:05:00|0.2776|751|  0|34.26|60.9|0.3|1.3| 124|25.65|\n",
      "|3603|21/05/13|12:10:00|0.2776|963|  0|35.18|59.6|0.3|1.3| 114|26.15|\n",
      "|3604|21/05/13|12:15:00|0.2776|956|  0| 35.8|57.5|0.3|1.3| 107|26.12|\n",
      "|3605|21/05/13|12:20:00|0.2776|994|  0|35.98|58.8|0.3|1.3| 105|26.68|\n",
      "+----+--------+--------+------+---+---+-----+----+---+---+----+-----+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Initialize Spark session\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"Read CSV from HDFS\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# Path to the CSV file on HDFS\n",
    "file_path = f'{MERGED_PATH}/combined_raw_data.csv'\n",
    "\n",
    "# Read the CSV file into a DataFrame\n",
    "df = spark.read.csv(file_path, header=True, inferSchema=False)\n",
    "\n",
    "df.printSchema()\n",
    "df.show(5)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Let change the col. name for readable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- _c0: string (nullable = true)\n",
      " |-- date: string (nullable = true)\n",
      " |-- time: string (nullable = true)\n",
      " |-- water_content(m3/m3): string (nullable = true)\n",
      " |-- solar_radiation(w/m2): string (nullable = true)\n",
      " |-- rain(mm): string (nullable = true)\n",
      " |-- temperature(celcius): string (nullable = true)\n",
      " |-- rh(%): string (nullable = true)\n",
      " |-- wind_speed(m/s): string (nullable = true)\n",
      " |-- gust_speed(m/s): string (nullable = true)\n",
      " |-- wind_direction(degree): string (nullable = true)\n",
      " |-- dew_point(celcius): string (nullable = true)\n",
      "\n",
      "root\n",
      " |-- _c0: string (nullable = true)\n",
      " |-- _c1: string (nullable = true)\n",
      " |-- _c2: string (nullable = true)\n",
      " |-- _c3: string (nullable = true)\n",
      " |-- _c4: string (nullable = true)\n",
      " |-- _c5: string (nullable = true)\n",
      " |-- _c6: string (nullable = true)\n",
      " |-- _c7: string (nullable = true)\n",
      " |-- _c8: string (nullable = true)\n",
      " |-- _c9: string (nullable = true)\n",
      " |-- _c10: string (nullable = true)\n",
      " |-- _c11: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# Assuming 'df' is your DataFrame\n",
    "# Rename columns\n",
    "column_mapping = {\n",
    "    \"_c1\": \"date\",\n",
    "    \"_c2\": \"time\",\n",
    "    \"_c3\": \"water_content(m3/m3)\",\n",
    "    \"_c4\": \"solar_radiation(w/m2)\",\n",
    "    \"_c5\": \"rain(mm)\",\n",
    "    \"_c6\": \"temperature(celcius)\",\n",
    "    \"_c7\": \"rh(%)\",\n",
    "    \"_c8\": \"wind_speed(m/s)\",\n",
    "    \"_c9\": \"gust_speed(m/s)\",\n",
    "    \"_c10\": \"wind_direction(degree)\",\n",
    "    \"_c11\": \"dew_point(celcius)\"\n",
    "}\n",
    "\n",
    "# Start with the original DataFrame\n",
    "rename_cf = df\n",
    "\n",
    "# Rename columns iteratively\n",
    "for old_name, new_name in column_mapping.items():\n",
    "    rename_cf = rename_cf.withColumnRenamed(old_name, new_name)\n",
    "\n",
    "# Show the DataFrame schema after renaming\n",
    "rename_cf.printSchema()\n",
    "df.printSchema()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Change the format date from yy/MM/dd to dd-MM-yyyy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+----------+--------+--------------------+---------------------+--------+--------------------+-----+---------------+---------------+----------------------+------------------+\n",
      "|_c0 |date      |time    |water_content(m3/m3)|solar_radiation(w/m2)|rain(mm)|temperature(celcius)|rh(%)|wind_speed(m/s)|gust_speed(m/s)|wind_direction(degree)|dew_point(celcius)|\n",
      "+----+----------+--------+--------------------+---------------------+--------+--------------------+-----+---------------+---------------+----------------------+------------------+\n",
      "|3601|13-05-2021|12:00:00|0.278               |233                  |0       |34.92               |60.8 |0.3            |1.7            |125                   |26.24             |\n",
      "|3602|13-05-2021|12:05:00|0.2776              |751                  |0       |34.26               |60.9 |0.3            |1.3            |124                   |25.65             |\n",
      "|3603|13-05-2021|12:10:00|0.2776              |963                  |0       |35.18               |59.6 |0.3            |1.3            |114                   |26.15             |\n",
      "|3604|13-05-2021|12:15:00|0.2776              |956                  |0       |35.8                |57.5 |0.3            |1.3            |107                   |26.12             |\n",
      "|3605|13-05-2021|12:20:00|0.2776              |994                  |0       |35.98               |58.8 |0.3            |1.3            |105                   |26.68             |\n",
      "+----+----------+--------+--------------------+---------------------+--------+--------------------+-----+---------------+---------------+----------------------+------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import functions as F\n",
    "\n",
    "# Reformatting the date and time columns\n",
    "df_transformed = rename_cf.withColumn(\"date\", F.date_format(F.to_date(\"date\", \"yy/MM/dd\"), \"dd-MM-yyyy\"))\n",
    "\n",
    "df_transformed.show(5, truncate=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Drop the dulpicate value from dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----+----+--------------------+---------------------+--------+--------------------+-----+---------------+---------------+----------------------+------------------+\n",
      "|_c0|date|time|water_content(m3/m3)|solar_radiation(w/m2)|rain(mm)|temperature(celcius)|rh(%)|wind_speed(m/s)|gust_speed(m/s)|wind_direction(degree)|dew_point(celcius)|\n",
      "+---+----+----+--------------------+---------------------+--------+--------------------+-----+---------------+---------------+----------------------+------------------+\n",
      "+---+----+----+--------------------+---------------------+--------+--------------------+-----+---------------+---------------+----------------------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Drop duplicates from the DataFrame\n",
    "df_no_duplicates = df_transformed.dropDuplicates()\n",
    "\n",
    "# Find the duplicate rows by subtracting deduplicated data from the original\n",
    "duplicates = df_transformed.subtract(df_no_duplicates)\n",
    "\n",
    "# Show the duplicated rows\n",
    "duplicates.show(5)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Drop _c0 Column "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop the \"_cO\" column\n",
    "drop_c0_colunm = df_no_duplicates.drop('_c0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- date: string (nullable = true)\n",
      " |-- time: string (nullable = true)\n",
      " |-- water_content(m3/m3): string (nullable = true)\n",
      " |-- solar_radiation(w/m2): string (nullable = true)\n",
      " |-- rain(mm): string (nullable = true)\n",
      " |-- temperature(celcius): string (nullable = true)\n",
      " |-- rh(%): string (nullable = true)\n",
      " |-- wind_speed(m/s): string (nullable = true)\n",
      " |-- gust_speed(m/s): string (nullable = true)\n",
      " |-- wind_direction(degree): string (nullable = true)\n",
      " |-- dew_point(celcius): string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "drop_c0_colunm.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Write it into hadoop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "drop_c0_colunm.write \\\n",
    "    .mode(\"overwrite\") \\\n",
    "    .option(\"header\", \"true\") \\\n",
    "    .option(\"compression\", \"none\") \\\n",
    "    .csv(f'{CLEANED_PATH}/cleaned_data.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Verify the file write successfully"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+--------+--------------------+---------------------+--------+--------------------+-----+---------------+---------------+----------------------+------------------+\n",
      "|      date|    time|water_content(m3/m3)|solar_radiation(w/m2)|rain(mm)|temperature(celcius)|rh(%)|wind_speed(m/s)|gust_speed(m/s)|wind_direction(degree)|dew_point(celcius)|\n",
      "+----------+--------+--------------------+---------------------+--------+--------------------+-----+---------------+---------------+----------------------+------------------+\n",
      "|15-05-2021|01:10:00|              0.2753|                    1|       0|               26.28| 89.5|              0|              0|               -888.88|             24.45|\n",
      "|13-05-2021|04:55:00|               0.278|                    1|       0|               26.43| 94.8|              0|              0|               -888.88|             25.56|\n",
      "|17-05-2021|04:45:00|              0.2739|                    1|       0|               27.16| 93.4|              0|              0|                   154|             26.04|\n",
      "|06-06-2021|13:20:00|             -0.4737|                    1|       0|              537.33|    0|              0|        -888.88|               -888.88|              -273|\n",
      "|06-06-2021|14:05:00|              0.2891|                  119|       0|               24.41|100.7|            0.3|            2.3|               -888.88|             24.41|\n",
      "+----------+--------+--------------------+---------------------+--------+--------------------+-----+---------------+---------------+----------------------+------------------+\n",
      "only showing top 5 rows\n",
      "\n",
      "root\n",
      " |-- date: string (nullable = true)\n",
      " |-- time: string (nullable = true)\n",
      " |-- water_content(m3/m3): string (nullable = true)\n",
      " |-- solar_radiation(w/m2): string (nullable = true)\n",
      " |-- rain(mm): string (nullable = true)\n",
      " |-- temperature(celcius): string (nullable = true)\n",
      " |-- rh(%): string (nullable = true)\n",
      " |-- wind_speed(m/s): string (nullable = true)\n",
      " |-- gust_speed(m/s): string (nullable = true)\n",
      " |-- wind_direction(degree): string (nullable = true)\n",
      " |-- dew_point(celcius): string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Initialize Spark session\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"Read CSV from HDFS\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# Path to the CSV file on HDFS\n",
    "file_path = f'{CLEANED_PATH}/cleaned_data.csv'\n",
    "\n",
    "# Read the CSV file into a DataFrame\n",
    "df = spark.read.csv(file_path, header=True)\n",
    "\n",
    "df.show(5)\n",
    "df.printSchema()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
