{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Partition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Import necessary modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from utils.Constants import Constants\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import year, month, concat_ws, col\n",
    "from pyspark.sql.types import StringType,DateType"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Configure the hadoop dir path and hive location"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cleaned path and hive location hdfs://localhost:8020/user/hadoop/cleaned , hdfs://localhost:8020/user/hive/warehouse\n"
     ]
    }
   ],
   "source": [
    "BASE_PATH = 'hdfs://localhost:8020/user'\n",
    "CLEANED_PATH = f'{BASE_PATH}/hadoop/cleaned'\n",
    "HIVE_DIR = 'hive/warehouse'\n",
    "HIVE_LOCATION = f'{BASE_PATH}/{HIVE_DIR}'\n",
    "\n",
    "print('cleaned path and hive location {} , {}'.format(CLEANED_PATH , HIVE_LOCATION))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Read the data that we have cleaned"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- date: string (nullable = true)\n",
      " |-- time: string (nullable = true)\n",
      " |-- water_content: float (nullable = true)\n",
      " |-- solar_radiation: float (nullable = true)\n",
      " |-- rain: float (nullable = true)\n",
      " |-- temperature: float (nullable = true)\n",
      " |-- rh: float (nullable = true)\n",
      " |-- wind_speed: float (nullable = true)\n",
      " |-- gust_speed: float (nullable = true)\n",
      " |-- wind_direction: float (nullable = true)\n",
      " |-- dew_point: float (nullable = true)\n",
      "\n",
      "+----------+--------+-------------+---------------+----+-----------+----+----------+----------+--------------+---------+\n",
      "|      date|    time|water_content|solar_radiation|rain|temperature|  rh|wind_speed|gust_speed|wind_direction|dew_point|\n",
      "+----------+--------+-------------+---------------+----+-----------+----+----------+----------+--------------+---------+\n",
      "|2020-10-23|01:15:00|       0.3069|            1.0| 0.0|      25.77|92.4|       0.0|       0.3|         277.0|    24.47|\n",
      "|2020-10-23|01:20:00|       0.3066|            1.0| 0.0|      25.74|92.4|       0.0|       0.7|         320.0|    24.45|\n",
      "|2020-10-23|01:25:00|       0.3069|            1.0| 0.0|      25.79|92.1|       0.3|       1.3|         309.0|    24.44|\n",
      "|2020-10-23|01:30:00|       0.3069|            1.0| 0.0|      25.74|92.1|       0.0|       0.3|         303.0|     24.4|\n",
      "|2020-10-23|01:35:00|       0.3066|            1.0| 0.0|       25.7|91.9|       0.0|       1.0|          14.0|    24.31|\n",
      "+----------+--------+-------------+---------------+----+-----------+----+----------+----------+--------------+---------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/02/01 16:53:35 WARN SparkSession: Using an existing Spark session; only runtime SQL configurations will take effect.\n"
     ]
    }
   ],
   "source": [
    "# Initialize Spark session\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"Read CSV from HDFS\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# Path to the CSV file on HDFS\n",
    "file_path = f'{CLEANED_PATH}/cleaned_data.csv'\n",
    "schema = Constants.WEATHER_DATA_SCHEMA\n",
    "\n",
    "# Read the CSV file into a DataFrame\n",
    "df = spark.read.csv(file_path, header=True , schema=Constants.WEATHER_DATA_SCHEMA)\n",
    "\n",
    "df.printSchema()\n",
    "df.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Let partition by date , becasue the col. date is not having high cardinality."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/02/01 16:53:35 WARN SparkSession: Using an existing Spark session; only runtime SQL configurations will take effect.\n",
      "25/02/01 16:53:36 WARN MemoryManager: Total allocation exceeds 95.00% (1,020,054,720 bytes) of heap memory\n",
      "Scaling row group sizes to 95.00% for 8 writers\n",
      "25/02/01 16:53:36 WARN MemoryManager: Total allocation exceeds 95.00% (1,020,054,720 bytes) of heap memory\n",
      "Scaling row group sizes to 84.44% for 9 writers\n",
      "25/02/01 16:53:36 WARN MemoryManager: Total allocation exceeds 95.00% (1,020,054,720 bytes) of heap memory\n",
      "Scaling row group sizes to 76.00% for 10 writers\n",
      "25/02/01 16:53:36 WARN MemoryManager: Total allocation exceeds 95.00% (1,020,054,720 bytes) of heap memory\n",
      "Scaling row group sizes to 69.09% for 11 writers\n",
      "25/02/01 16:53:36 WARN MemoryManager: Total allocation exceeds 95.00% (1,020,054,720 bytes) of heap memory\n",
      "Scaling row group sizes to 63.33% for 12 writers\n",
      "25/02/01 16:53:36 WARN MemoryManager: Total allocation exceeds 95.00% (1,020,054,720 bytes) of heap memory\n",
      "Scaling row group sizes to 69.09% for 11 writers\n",
      "25/02/01 16:53:36 WARN MemoryManager: Total allocation exceeds 95.00% (1,020,054,720 bytes) of heap memory\n",
      "Scaling row group sizes to 76.00% for 10 writers\n",
      "25/02/01 16:53:36 WARN MemoryManager: Total allocation exceeds 95.00% (1,020,054,720 bytes) of heap memory\n",
      "Scaling row group sizes to 69.09% for 11 writers\n",
      "25/02/01 16:53:36 WARN MemoryManager: Total allocation exceeds 95.00% (1,020,054,720 bytes) of heap memory\n",
      "Scaling row group sizes to 63.33% for 12 writers\n",
      "25/02/01 16:53:36 WARN MemoryManager: Total allocation exceeds 95.00% (1,020,054,720 bytes) of heap memory\n",
      "Scaling row group sizes to 69.09% for 11 writers\n",
      "25/02/01 16:53:36 WARN MemoryManager: Total allocation exceeds 95.00% (1,020,054,720 bytes) of heap memory\n",
      "Scaling row group sizes to 76.00% for 10 writers\n",
      "25/02/01 16:53:36 WARN MemoryManager: Total allocation exceeds 95.00% (1,020,054,720 bytes) of heap memory\n",
      "Scaling row group sizes to 84.44% for 9 writers\n",
      "25/02/01 16:53:36 WARN MemoryManager: Total allocation exceeds 95.00% (1,020,054,720 bytes) of heap memory\n",
      "Scaling row group sizes to 95.00% for 8 writers\n",
      "25/02/01 16:53:36 WARN MemoryManager: Total allocation exceeds 95.00% (1,020,054,720 bytes) of heap memory\n",
      "Scaling row group sizes to 95.00% for 8 writers\n",
      "25/02/01 16:53:36 WARN MemoryManager: Total allocation exceeds 95.00% (1,020,054,720 bytes) of heap memory\n",
      "Scaling row group sizes to 84.44% for 9 writers\n",
      "25/02/01 16:53:36 WARN MemoryManager: Total allocation exceeds 95.00% (1,020,054,720 bytes) of heap memory\n",
      "Scaling row group sizes to 76.00% for 10 writers\n",
      "25/02/01 16:53:36 WARN MemoryManager: Total allocation exceeds 95.00% (1,020,054,720 bytes) of heap memory\n",
      "Scaling row group sizes to 69.09% for 11 writers\n",
      "25/02/01 16:53:36 WARN MemoryManager: Total allocation exceeds 95.00% (1,020,054,720 bytes) of heap memory\n",
      "Scaling row group sizes to 63.33% for 12 writers\n",
      "25/02/01 16:53:36 WARN MemoryManager: Total allocation exceeds 95.00% (1,020,054,720 bytes) of heap memory\n",
      "Scaling row group sizes to 69.09% for 11 writers\n",
      "25/02/01 16:53:36 WARN MemoryManager: Total allocation exceeds 95.00% (1,020,054,720 bytes) of heap memory\n",
      "Scaling row group sizes to 63.33% for 12 writers\n",
      "25/02/01 16:53:36 WARN MemoryManager: Total allocation exceeds 95.00% (1,020,054,720 bytes) of heap memory\n",
      "Scaling row group sizes to 69.09% for 11 writers\n",
      "25/02/01 16:53:36 WARN MemoryManager: Total allocation exceeds 95.00% (1,020,054,720 bytes) of heap memory\n",
      "Scaling row group sizes to 76.00% for 10 writers\n",
      "25/02/01 16:53:36 WARN MemoryManager: Total allocation exceeds 95.00% (1,020,054,720 bytes) of heap memory\n",
      "Scaling row group sizes to 84.44% for 9 writers\n",
      "25/02/01 16:53:36 WARN MemoryManager: Total allocation exceeds 95.00% (1,020,054,720 bytes) of heap memory\n",
      "Scaling row group sizes to 76.00% for 10 writers\n",
      "25/02/01 16:53:36 WARN MemoryManager: Total allocation exceeds 95.00% (1,020,054,720 bytes) of heap memory\n",
      "Scaling row group sizes to 84.44% for 9 writers\n",
      "25/02/01 16:53:36 WARN MemoryManager: Total allocation exceeds 95.00% (1,020,054,720 bytes) of heap memory\n",
      "Scaling row group sizes to 95.00% for 8 writers\n",
      "25/02/01 16:53:36 WARN MemoryManager: Total allocation exceeds 95.00% (1,020,054,720 bytes) of heap memory\n",
      "Scaling row group sizes to 84.44% for 9 writers\n",
      "25/02/01 16:53:36 WARN MemoryManager: Total allocation exceeds 95.00% (1,020,054,720 bytes) of heap memory\n",
      "Scaling row group sizes to 76.00% for 10 writers\n",
      "25/02/01 16:53:36 WARN MemoryManager: Total allocation exceeds 95.00% (1,020,054,720 bytes) of heap memory\n",
      "Scaling row group sizes to 84.44% for 9 writers\n",
      "25/02/01 16:53:36 WARN MemoryManager: Total allocation exceeds 95.00% (1,020,054,720 bytes) of heap memory\n",
      "Scaling row group sizes to 95.00% for 8 writers\n",
      "25/02/01 16:53:36 WARN MemoryManager: Total allocation exceeds 95.00% (1,020,054,720 bytes) of heap memory\n",
      "Scaling row group sizes to 95.00% for 8 writers\n",
      "25/02/01 16:53:36 WARN MemoryManager: Total allocation exceeds 95.00% (1,020,054,720 bytes) of heap memory\n",
      "Scaling row group sizes to 84.44% for 9 writers\n",
      "25/02/01 16:53:36 WARN MemoryManager: Total allocation exceeds 95.00% (1,020,054,720 bytes) of heap memory\n",
      "Scaling row group sizes to 95.00% for 8 writers\n",
      "25/02/01 16:53:36 WARN MemoryManager: Total allocation exceeds 95.00% (1,020,054,720 bytes) of heap memory\n",
      "Scaling row group sizes to 84.44% for 9 writers\n",
      "25/02/01 16:53:36 WARN MemoryManager: Total allocation exceeds 95.00% (1,020,054,720 bytes) of heap memory\n",
      "Scaling row group sizes to 95.00% for 8 writers\n",
      "[Stage 5:====================================================>    (11 + 1) / 12]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data has been partitioned by month and saved to Hive table 'weatherdb.weather_data_partitioned_by_month'.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import year, month, concat_ws, col\n",
    "from pyspark.sql.types import StringType, DateType\n",
    "\n",
    "# Initialize Spark session with Hive support\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"Python Spark SQL Hive integration example\") \\\n",
    "    .config(\"spark.sql.warehouse.dir\", HIVE_LOCATION) \\\n",
    "    .config(\"hive.metastore.uris\", \"thrift://hive-metastore:9083\") \\\n",
    "    .config(\"spark.executor.memory\", \"4g\") \\\n",
    "    .config(\"spark.driver.memory\", \"2g\") \\\n",
    "    .config(\"spark.sql.shuffle.partitions\", \"50\") \\\n",
    "    .config(\"spark.memory.fraction\", \"0.7\") \\\n",
    "    .enableHiveSupport() \\\n",
    "    .getOrCreate()\n",
    "\n",
    "\n",
    "# Create database if it doesn't exist\n",
    "spark.sql(\"CREATE DATABASE IF NOT EXISTS weatherdb\")\n",
    "spark.sql(\"USE weatherdb\")\n",
    "\n",
    "# Read the cleaned data\n",
    "file_path = f\"{CLEANED_PATH}/cleaned_data.csv\"\n",
    "data = spark.read.csv(file_path, header=True, schema=Constants.WEATHER_DATA_SCHEMA)\n",
    "\n",
    "# Cast columns to appropriate types\n",
    "data = data.withColumn(\"date\", col(\"date\").cast(DateType()))\n",
    "data = data.withColumn(\"time\", col(\"time\").cast(StringType()))\n",
    "\n",
    "# Add a year_month column for partitioning\n",
    "data = data.withColumn(\"year_month\", concat_ws(\"-\", year(data[\"date\"]), month(data[\"date\"])))\n",
    "\n",
    "# Save the data as a partitioned Parquet file\n",
    "data.write \\\n",
    "    .format(\"parquet\") \\\n",
    "    .mode(\"overwrite\") \\\n",
    "    .partitionBy(\"year_month\") \\\n",
    "    .save(f\"{HIVE_LOCATION}/weather_data_partitioned_by_month\")\n",
    "\n",
    "# Define the schema for the Hive table\n",
    "schema = \", \".join([f\"{field.name} {field.dataType.simpleString()}\" for field in data.schema.fields if field.name != \"year_month\"])\n",
    "\n",
    "# Register the Parquet files as a Hive table with explicit schema\n",
    "spark.sql(f\"\"\"\n",
    "    CREATE TABLE IF NOT EXISTS weatherdb.weather_data_partitioned_by_month (\n",
    "        {schema}\n",
    "    )\n",
    "    USING parquet\n",
    "    PARTITIONED BY (year_month STRING)\n",
    "    LOCATION 'hdfs://localhost:8020/user/hive/warehouse/weather_data_partitioned_by_month'\n",
    "\"\"\")\n",
    "\n",
    "# Refresh the table metadata to recognize the partitions\n",
    "spark.sql(\"MSCK REPAIR TABLE weatherdb.weather_data_partitioned_by_month\")\n",
    "\n",
    "print(\"Data has been partitioned by month and saved to Hive table 'weatherdb.weather_data_partitioned_by_month'.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Describe the table schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/02/01 16:53:38 WARN SparkSession: Using an existing Spark session; only runtime SQL configurations will take effect.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+---------+-------+\n",
      "|            col_name|data_type|comment|\n",
      "+--------------------+---------+-------+\n",
      "|                date|     date|   NULL|\n",
      "|                time|   string|   NULL|\n",
      "|       water_content|    float|   NULL|\n",
      "|     solar_radiation|    float|   NULL|\n",
      "|                rain|    float|   NULL|\n",
      "|         temperature|    float|   NULL|\n",
      "|                  rh|    float|   NULL|\n",
      "|          wind_speed|    float|   NULL|\n",
      "|          gust_speed|    float|   NULL|\n",
      "|      wind_direction|    float|   NULL|\n",
      "|           dew_point|    float|   NULL|\n",
      "|          year_month|   string|   NULL|\n",
      "|# Partition Infor...|         |       |\n",
      "|          # col_name|data_type|comment|\n",
      "|          year_month|   string|   NULL|\n",
      "+--------------------+---------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "spark = SparkSession \\\n",
    "    .builder \\\n",
    "    .appName(\"Python Spark SQL Hive integration example\") \\\n",
    "    .config(\"spark.sql.warehouse.dir\", HIVE_LOCATION) \\\n",
    "    .config(\"hive.metastore.uris\", \"thrift://hive-metastore:9083\") \\\n",
    "    .enableHiveSupport() \\\n",
    "    .getOrCreate()\n",
    "\n",
    "spark.sql(\"DESCRIBE weatherdb.weather_data_partitioned_by_month\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Test Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/02/01 16:53:38 WARN SparkSession: Using an existing Spark session; only runtime SQL configurations will take effect.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+--------+-------------+---------------+----+-----------+----+----------+----------+--------------+---------+----------+\n",
      "|      date|    time|water_content|solar_radiation|rain|temperature|  rh|wind_speed|gust_speed|wind_direction|dew_point|year_month|\n",
      "+----------+--------+-------------+---------------+----+-----------+----+----------+----------+--------------+---------+----------+\n",
      "|2020-11-01|00:00:00|       0.3062|            1.0| 0.0|      26.84|94.8|       0.3|       1.0|         152.0|    25.97|   2020-11|\n",
      "|2020-11-01|00:05:00|       0.3066|            1.0| 0.0|      26.77|94.7|       0.0|       1.0|         152.0|    25.88|   2020-11|\n",
      "|2020-11-01|00:10:00|       0.3066|            1.0| 0.0|      26.72|94.7|       0.3|       1.3|         154.0|    25.83|   2020-11|\n",
      "|2020-11-01|00:15:00|       0.3066|            1.0| 0.0|      26.65|94.6|       0.0|       0.0|         152.0|    25.74|   2020-11|\n",
      "|2020-11-01|00:20:00|       0.3066|            1.0| 0.0|      26.55|94.7|       0.0|       0.0|         152.0|    25.66|   2020-11|\n",
      "+----------+--------+-------------+---------------+----+-----------+----+----------+----------+--------------+---------+----------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark = SparkSession \\\n",
    "    .builder \\\n",
    "    .appName(\"Python Spark SQL Hive integration example\") \\\n",
    "    .config(\"spark.sql.warehouse.dir\", HIVE_DIR) \\\n",
    "    .enableHiveSupport() \\\n",
    "    .getOrCreate()\n",
    "\n",
    "result = spark.sql(\"\"\"\n",
    "SELECT * \n",
    "FROM weatherdb.weather_data_partitioned_by_month\n",
    "\"\"\")\n",
    "result.show(5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+--------------------+-----------+\n",
      "|namespace|           tableName|isTemporary|\n",
      "+---------+--------------------+-----------+\n",
      "|weatherdb|weather_data_part...|      false|\n",
      "+---------+--------------------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"SHOW TABLES IN weatherdb\").show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
