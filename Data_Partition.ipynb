{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Partition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Import necessary modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from utils.Constants import Constants\n",
    "from pyspark.sql.functions import month\n",
    "from os.path import abspath\n",
    "from pyspark.sql.functions import month, year, concat_ws"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Configure the hadoop dir path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cleand path hdfs://localhost:9000/user/hadoop/cleaned\n"
     ]
    }
   ],
   "source": [
    "BASE_PATH = 'hdfs://localhost:9000/user/hadoop'\n",
    "CLEANED_PATH = f'{BASE_PATH}/cleaned'\n",
    "PARTITION_PATH = f'{BASE_PATH}/partitions'\n",
    "\n",
    "print('cleand path {}'.format(CLEANED_PATH))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Read the data that we have cleaned"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- date: date (nullable = true)\n",
      " |-- time: string (nullable = true)\n",
      " |-- water_content(m3/m3): float (nullable = true)\n",
      " |-- solar_radiation(w/m2): float (nullable = true)\n",
      " |-- rain(mm): float (nullable = true)\n",
      " |-- temperature(celcius): float (nullable = true)\n",
      " |-- rh(%): float (nullable = true)\n",
      " |-- wind_speed(m/s): float (nullable = true)\n",
      " |-- gust_speed(m/s): float (nullable = true)\n",
      " |-- wind_direction(degree): float (nullable = true)\n",
      " |-- dew_point(celcius): float (nullable = true)\n",
      "\n",
      "+----------+--------+--------------------+---------------------+--------+--------------------+-----+---------------+---------------+----------------------+------------------+\n",
      "|      date|    time|water_content(m3/m3)|solar_radiation(w/m2)|rain(mm)|temperature(celcius)|rh(%)|wind_speed(m/s)|gust_speed(m/s)|wind_direction(degree)|dew_point(celcius)|\n",
      "+----------+--------+--------------------+---------------------+--------+--------------------+-----+---------------+---------------+----------------------+------------------+\n",
      "|2021-05-13|17:25:00|               0.277|                 81.0|     0.0|               33.11| 67.5|            0.3|            2.3|               -888.88|             26.31|\n",
      "|2021-05-14|09:15:00|              0.2759|                579.0|     0.0|               32.79| 77.2|            0.0|            0.3|               -888.88|             28.31|\n",
      "|2021-05-14|14:50:00|              0.2759|                376.0|     0.0|               34.65| 63.7|            1.0|            2.7|               -888.88|             26.79|\n",
      "|2021-05-14|16:25:00|              0.2759|                317.0|     0.0|               27.95| 63.2|            0.3|            1.3|                 122.0|             20.33|\n",
      "|2021-05-14|18:20:00|              0.2753|                  1.0|     0.0|               30.85| 76.5|            0.0|            1.3|               -888.88|             26.27|\n",
      "+----------+--------+--------------------+---------------------+--------+--------------------+-----+---------------+---------------+----------------------+------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/12/07 14:24:35 WARN SparkSession: Using an existing Spark session; only runtime SQL configurations will take effect.\n"
     ]
    }
   ],
   "source": [
    "# Initialize Spark session\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"Read CSV from HDFS\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# Path to the CSV file on HDFS\n",
    "file_path = f'{CLEANED_PATH}/cleaned_data.csv'\n",
    "schema = Constants.WEATHER_DATA_SCHEMA\n",
    "\n",
    "# Read the CSV file into a DataFrame\n",
    "df = spark.read.csv(file_path, header=True , schema=schema)\n",
    "\n",
    "df.printSchema()\n",
    "df.show(5)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Let partition by date , becasue the col. date is not having high cardinality."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/12/07 14:24:35 WARN SparkSession: Using an existing Spark session; only runtime SQL configurations will take effect.\n",
      "[Stage 5:===================================>                       (3 + 2) / 5]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data has been partitioned by month and saved to Hive table 'mydb.weather_data_partitioned_by_month'.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "warehouse_location = abspath('spark-warehouse')\n",
    "schema = Constants.WEATHER_DATA_SCHEMA\n",
    "\n",
    "spark = SparkSession \\\n",
    "    .builder \\\n",
    "    .appName(\"Python Spark SQL Hive integration example\") \\\n",
    "    .config(\"spark.sql.warehouse.dir\", warehouse_location) \\\n",
    "    .config(\"spark.sql.catalogImplementation\", \"hive\") \\\n",
    "    .enableHiveSupport() \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# Read the CSV file with the defined schema\n",
    "file_path = f'{CLEANED_PATH}/cleaned_data.csv'\n",
    "schema = Constants.WEATHER_DATA_SCHEMA\n",
    "\n",
    "# Read the CSV file into a DataFrame\n",
    "data = spark.read.csv(file_path, header=True , schema=schema)\n",
    "spark.sql(\"CREATE DATABASE IF NOT EXISTS mydb\")\n",
    "\n",
    "data = data \\\n",
    "    .withColumn(\"year_month\", concat_ws(\"-\", year(data[\"date\"]), month(data[\"date\"]))) \\\n",
    "    .orderBy(\"date\", \"time\")\n",
    "\n",
    "data.write \\\n",
    "    .format(\"parquet\") \\\n",
    "    .mode(\"overwrite\") \\\n",
    "    .partitionBy(\"year_month\") \\\n",
    "    .saveAsTable(\"mydb.weather_data_partitioned_by_month\")\n",
    "\n",
    "print(f\"Data has been partitioned by month and saved to Hive table 'mydb.weather_data_partitioned_by_month'.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Describe the table schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+---------+-------+\n",
      "|            col_name|data_type|comment|\n",
      "+--------------------+---------+-------+\n",
      "|                date|     date|   NULL|\n",
      "|                time|   string|   NULL|\n",
      "|water_content(m3/m3)|    float|   NULL|\n",
      "|solar_radiation(w...|    float|   NULL|\n",
      "|            rain(mm)|    float|   NULL|\n",
      "|temperature(celcius)|    float|   NULL|\n",
      "|               rh(%)|    float|   NULL|\n",
      "|     wind_speed(m/s)|    float|   NULL|\n",
      "|     gust_speed(m/s)|    float|   NULL|\n",
      "|wind_direction(de...|    float|   NULL|\n",
      "|  dew_point(celcius)|    float|   NULL|\n",
      "|          year_month|   string|   NULL|\n",
      "|# Partition Infor...|         |       |\n",
      "|          # col_name|data_type|comment|\n",
      "|          year_month|   string|   NULL|\n",
      "+--------------------+---------+-------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/12/07 14:24:43 WARN SparkSession: Using an existing Spark session; only runtime SQL configurations will take effect.\n"
     ]
    }
   ],
   "source": [
    "warehouse_location = abspath('spark-warehouse')\n",
    "\n",
    "spark = SparkSession \\\n",
    "    .builder \\\n",
    "    .appName(\"Python Spark SQL Hive integration example\") \\\n",
    "    .config(\"spark.sql.warehouse.dir\", warehouse_location) \\\n",
    "    .config(\"spark.sql.catalogImplementation\", \"hive\") \\\n",
    "    .enableHiveSupport() \\\n",
    "    .getOrCreate()\n",
    "\n",
    "spark.sql(\"DESCRIBE mydb.weather_data_partitioned_by_month\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Test Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+--------+--------------------+---------------------+--------+--------------------+-----+---------------+---------------+----------------------+------------------+----------+\n",
      "|      date|    time|water_content(m3/m3)|solar_radiation(w/m2)|rain(mm)|temperature(celcius)|rh(%)|wind_speed(m/s)|gust_speed(m/s)|wind_direction(degree)|dew_point(celcius)|year_month|\n",
      "+----------+--------+--------------------+---------------------+--------+--------------------+-----+---------------+---------------+----------------------+------------------+----------+\n",
      "|2021-01-01|15:35:00|              0.3393|                221.0|     0.0|               26.57| 60.9|            1.7|            4.0|                 331.0|             18.43|    2021-1|\n",
      "|2021-01-01|01:25:00|              0.3371|                  1.0|     0.0|               23.45| 63.4|            1.3|            3.7|                  21.0|             16.12|    2021-1|\n",
      "|2021-01-01|02:50:00|              0.3371|                  1.0|     0.0|                22.8| 67.1|            1.3|            4.0|                  24.0|              16.4|    2021-1|\n",
      "|2021-01-01|07:15:00|               0.338|                 89.0|     0.0|               21.96| 72.3|            0.7|            3.3|                  18.0|             16.77|    2021-1|\n",
      "|2021-01-01|11:15:00|              0.3384|                759.0|     0.0|               26.16| 59.4|            2.7|            8.0|                  21.0|             17.64|    2021-1|\n",
      "+----------+--------+--------------------+---------------------+--------+--------------------+-----+---------------+---------------+----------------------+------------------+----------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "result = spark.sql(\"\"\"\n",
    "SELECT * \n",
    "FROM mydb.weather_data_partitioned_by_month\n",
    "\"\"\")\n",
    "result.show(5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+--------------------+-----------+\n",
      "|namespace|           tableName|isTemporary|\n",
      "+---------+--------------------+-----------+\n",
      "|     mydb|weather_data_part...|      false|\n",
      "+---------+--------------------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"SHOW TABLES IN mydb\").show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
