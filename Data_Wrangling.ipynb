{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Wrangling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Import necessary modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import StructType, StructField, StringType, FloatType\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Configure the hadoop dir path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input path hdfs://localhost:9000/user/hadoop/inputs , merge path hdfs://localhost:9000/user/hadoop/merged\n"
     ]
    }
   ],
   "source": [
    "BASE_PATH = 'hdfs://localhost:9000/user/hadoop'\n",
    "INPUT_PATH = f'{BASE_PATH}/inputs'\n",
    "MERGED_PATH = f'{BASE_PATH}/merged'\n",
    "\n",
    "print('input path {} , merge path {}'.format(INPUT_PATH, MERGED_PATH))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Function for create spark session that connect to hadoop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_spark_session():\n",
    "    \"\"\"\n",
    "    Creates and configures a SparkSession with minimal memory settings.\n",
    "    \"\"\"\n",
    "    spark = SparkSession.builder \\\n",
    "    .appName(\"Weather Data Combination\") \\\n",
    "    .config(\"spark.jars.packages\", \"com.crealytics:spark-excel_2.12:0.13.7\") \\\n",
    "    .config(\"spark.driver.memory\", \"2g\") \\\n",
    "    .config(\"spark.executor.memory\", \"2g\") \\\n",
    "    .config(\"spark.driver.maxResultSize\", \"1g\") \\\n",
    "    .config(\"spark.ui.showConsoleProgress\", \"true\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "    return spark\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Create a function to load data from hadoop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_excel_files(spark, file_path):\n",
    "    \"\"\"\n",
    "    Load all Excel files from the given base path.\n",
    "    \"\"\"\n",
    "    df = spark.read.format(\"com.crealytics.spark.excel\") \\\n",
    "                .option(\"header\", \"false\") \\\n",
    "                .option(\"dataAddress\", \"'RUA Data'!A6\") \\\n",
    "                .option(\"maxRowsInMemory\", 1000) \\\n",
    "                .option(\"treatEmptyValuesAsNulls\", \"true\") \\\n",
    "                .load(file_path)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Test read data as xlsx file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- _c0: string (nullable = true)\n",
      " |-- _c1: string (nullable = true)\n",
      " |-- _c2: string (nullable = true)\n",
      " |-- _c3: string (nullable = true)\n",
      " |-- _c4: string (nullable = true)\n",
      " |-- _c5: string (nullable = true)\n",
      " |-- _c6: string (nullable = true)\n",
      " |-- _c7: string (nullable = true)\n",
      " |-- _c8: string (nullable = true)\n",
      " |-- _c9: string (nullable = true)\n",
      " |-- _c10: string (nullable = true)\n",
      " |-- _c11: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark = create_spark_session()\n",
    "hdfs_path = f'{INPUT_PATH}/APRIL-2021.xlsx'\n",
    "df = load_excel_files(spark, hdfs_path)\n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Configure the files that we need to work with"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['hdfs://localhost:9000/user/hadoop/inputs/APRIL-2021.xlsx', 'hdfs://localhost:9000/user/hadoop/inputs/APRIL-2022.xlsx', 'hdfs://localhost:9000/user/hadoop/inputs/AUGUST-2021.xlsx', 'hdfs://localhost:9000/user/hadoop/inputs/DECEMBER-2020.xlsx', 'hdfs://localhost:9000/user/hadoop/inputs/DECEMBER-2021.xlsx', 'hdfs://localhost:9000/user/hadoop/inputs/FEBRUARY-2021.xlsx', 'hdfs://localhost:9000/user/hadoop/inputs/FEBRUARY-2022.xlsx', 'hdfs://localhost:9000/user/hadoop/inputs/JANUARY-2021.xlsx', 'hdfs://localhost:9000/user/hadoop/inputs/JANUARY-2022.xlsx', 'hdfs://localhost:9000/user/hadoop/inputs/JULY-2021.xlsx', 'hdfs://localhost:9000/user/hadoop/inputs/MARCH-2021.xlsx', 'hdfs://localhost:9000/user/hadoop/inputs/MARCH-2022.xlsx', 'hdfs://localhost:9000/user/hadoop/inputs/MAY-2021.xlsx', 'hdfs://localhost:9000/user/hadoop/inputs/NOVEMBER-2020.xlsx', 'hdfs://localhost:9000/user/hadoop/inputs/NOVEMBER-2021.xlsx', 'hdfs://localhost:9000/user/hadoop/inputs/OCTOBER-2020.xlsx', 'hdfs://localhost:9000/user/hadoop/inputs/OCTOBER-2021.xlsx', 'hdfs://localhost:9000/user/hadoop/inputs/SEPTEMBER-2020.xlsx', 'hdfs://localhost:9000/user/hadoop/inputs/SEPTEMBER-2021.xlsx', 'hdfs://localhost:9000/user/hadoop/inputs/jUNE-2021.xlsx']\n"
     ]
    }
   ],
   "source": [
    "file_paths = [f\"{INPUT_PATH}/{filename}\" for filename in [\n",
    "    \"APRIL-2021.xlsx\", \"APRIL-2022.xlsx\", \"AUGUST-2021.xlsx\",\n",
    "    \"DECEMBER-2020.xlsx\", \"DECEMBER-2021.xlsx\", \"FEBRUARY-2021.xlsx\",\n",
    "    \"FEBRUARY-2022.xlsx\", \"JANUARY-2021.xlsx\", \"JANUARY-2022.xlsx\",\n",
    "    \"JULY-2021.xlsx\", \"MARCH-2021.xlsx\", \"MARCH-2022.xlsx\",\n",
    "    \"MAY-2021.xlsx\", \"NOVEMBER-2020.xlsx\", \"NOVEMBER-2021.xlsx\",\n",
    "    \"OCTOBER-2020.xlsx\", \"OCTOBER-2021.xlsx\", \"SEPTEMBER-2020.xlsx\",\n",
    "    \"SEPTEMBER-2021.xlsx\", \"jUNE-2021.xlsx\"\n",
    "]]\n",
    "print(file_paths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing file 1/20: hdfs://localhost:9000/user/hadoop/inputs/APRIL-2021.xlsx\n",
      "Successfully processed hdfs://localhost:9000/user/hadoop/inputs/APRIL-2021.xlsx\n",
      "Processing file 2/20: hdfs://localhost:9000/user/hadoop/inputs/APRIL-2022.xlsx\n",
      "Successfully processed hdfs://localhost:9000/user/hadoop/inputs/APRIL-2022.xlsx\n",
      "Processing file 3/20: hdfs://localhost:9000/user/hadoop/inputs/AUGUST-2021.xlsx\n",
      "Successfully processed hdfs://localhost:9000/user/hadoop/inputs/AUGUST-2021.xlsx\n",
      "Processing file 4/20: hdfs://localhost:9000/user/hadoop/inputs/DECEMBER-2020.xlsx\n",
      "Successfully processed hdfs://localhost:9000/user/hadoop/inputs/DECEMBER-2020.xlsx\n",
      "Processing file 5/20: hdfs://localhost:9000/user/hadoop/inputs/DECEMBER-2021.xlsx\n",
      "Successfully processed hdfs://localhost:9000/user/hadoop/inputs/DECEMBER-2021.xlsx\n",
      "Processing file 6/20: hdfs://localhost:9000/user/hadoop/inputs/FEBRUARY-2021.xlsx\n",
      "Successfully processed hdfs://localhost:9000/user/hadoop/inputs/FEBRUARY-2021.xlsx\n",
      "Processing file 7/20: hdfs://localhost:9000/user/hadoop/inputs/FEBRUARY-2022.xlsx\n",
      "Successfully processed hdfs://localhost:9000/user/hadoop/inputs/FEBRUARY-2022.xlsx\n",
      "Processing file 8/20: hdfs://localhost:9000/user/hadoop/inputs/JANUARY-2021.xlsx\n",
      "Successfully processed hdfs://localhost:9000/user/hadoop/inputs/JANUARY-2021.xlsx\n",
      "Processing file 9/20: hdfs://localhost:9000/user/hadoop/inputs/JANUARY-2022.xlsx\n",
      "Successfully processed hdfs://localhost:9000/user/hadoop/inputs/JANUARY-2022.xlsx\n",
      "Processing file 10/20: hdfs://localhost:9000/user/hadoop/inputs/JULY-2021.xlsx\n",
      "Successfully processed hdfs://localhost:9000/user/hadoop/inputs/JULY-2021.xlsx\n",
      "Processing file 11/20: hdfs://localhost:9000/user/hadoop/inputs/MARCH-2021.xlsx\n",
      "Successfully processed hdfs://localhost:9000/user/hadoop/inputs/MARCH-2021.xlsx\n",
      "Processing file 12/20: hdfs://localhost:9000/user/hadoop/inputs/MARCH-2022.xlsx\n",
      "Successfully processed hdfs://localhost:9000/user/hadoop/inputs/MARCH-2022.xlsx\n",
      "Processing file 13/20: hdfs://localhost:9000/user/hadoop/inputs/MAY-2021.xlsx\n",
      "Successfully processed hdfs://localhost:9000/user/hadoop/inputs/MAY-2021.xlsx\n",
      "Processing file 14/20: hdfs://localhost:9000/user/hadoop/inputs/NOVEMBER-2020.xlsx\n",
      "Successfully processed hdfs://localhost:9000/user/hadoop/inputs/NOVEMBER-2020.xlsx\n",
      "Processing file 15/20: hdfs://localhost:9000/user/hadoop/inputs/NOVEMBER-2021.xlsx\n",
      "Successfully processed hdfs://localhost:9000/user/hadoop/inputs/NOVEMBER-2021.xlsx\n",
      "Processing file 16/20: hdfs://localhost:9000/user/hadoop/inputs/OCTOBER-2020.xlsx\n",
      "Successfully processed hdfs://localhost:9000/user/hadoop/inputs/OCTOBER-2020.xlsx\n",
      "Processing file 17/20: hdfs://localhost:9000/user/hadoop/inputs/OCTOBER-2021.xlsx\n",
      "Successfully processed hdfs://localhost:9000/user/hadoop/inputs/OCTOBER-2021.xlsx\n",
      "Processing file 18/20: hdfs://localhost:9000/user/hadoop/inputs/SEPTEMBER-2020.xlsx\n",
      "Successfully processed hdfs://localhost:9000/user/hadoop/inputs/SEPTEMBER-2020.xlsx\n",
      "Processing file 19/20: hdfs://localhost:9000/user/hadoop/inputs/SEPTEMBER-2021.xlsx\n",
      "Successfully processed hdfs://localhost:9000/user/hadoop/inputs/SEPTEMBER-2021.xlsx\n",
      "Processing file 20/20: hdfs://localhost:9000/user/hadoop/inputs/jUNE-2021.xlsx\n",
      "Successfully processed hdfs://localhost:9000/user/hadoop/inputs/jUNE-2021.xlsx\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/11/30 14:33:20 WARN TaskSetManager: Stage 0 contains a task of very large size (19309 KiB). The maximum recommended task size is 1000 KiB.\n",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data combination completed successfully\n"
     ]
    }
   ],
   "source": [
    "# Stop any existing SparkSession\n",
    "if 'spark' in locals():\n",
    "    spark.stop()\n",
    "    \n",
    "# Create new SparkSession with minimal memory settings\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"Weather Data Combination\") \\\n",
    "    .config(\"spark.jars.packages\", \"com.crealytics:spark-excel_2.12:0.13.7\") \\\n",
    "    .config(\"spark.driver.memory\", \"2g\") \\\n",
    "    .config(\"spark.executor.memory\", \"2g\") \\\n",
    "    .config(\"spark.sql.shuffle.partitions\", \"2\") \\\n",
    "    .config(\"spark.driver.maxResultSize\", \"1g\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# Define schema with all required columns\n",
    "schema = StructType([\n",
    "    StructField(\"Line#\", StringType(), True),\n",
    "    StructField(\"Date\", StringType(), True),\n",
    "    StructField(\"Time\", StringType(), True),\n",
    "    StructField(\"Water Content (m3/m3)\", FloatType(), True),\n",
    "    StructField(\"Solar Radiation (W/m2)\", FloatType(), True),\n",
    "    StructField(\"Rain (mm)\", FloatType(), True),\n",
    "    StructField(\"Temperature (Celcius)\", FloatType(), True),\n",
    "    StructField(\"RH (%)\", FloatType(), True),\n",
    "    StructField(\"Wind Speed (m/s)\", FloatType(), True),\n",
    "    StructField(\"Gust Speed (m/s)\", FloatType(), True),\n",
    "    StructField(\"Wind Direction (Degree)\", FloatType(), True),\n",
    "    StructField(\"Dew Point (Celcius)\", FloatType(), True)\n",
    "])\n",
    "\n",
    "\n",
    "def process_and_combine_files(file_paths, schema, output_path):\n",
    "    \"\"\"\n",
    "    Process multiple Excel files and combine them into a single CSV file\n",
    "    while ensuring all columns are present and properly typed.\n",
    "    \"\"\"\n",
    "    combined_df = None\n",
    "    \n",
    "    for i, file_path in enumerate(file_paths):\n",
    "        try:\n",
    "            print(f\"Processing file {i+1}/{len(file_paths)}: {file_path}\")\n",
    "            \n",
    "            # Read Excel file\n",
    "            current_df = spark.read.format(\"com.crealytics.spark.excel\") \\\n",
    "                .option(\"header\", \"true\") \\\n",
    "                .option(\"dataAddress\", \"'RUA Data'!A6\") \\\n",
    "                .option(\"maxRowsInMemory\", 1000) \\\n",
    "                .option(\"treatEmptyValuesAsNulls\", \"true\") \\\n",
    "                .schema(schema) \\\n",
    "                .load(file_path)\n",
    "            \n",
    "            # Select columns in specific order to ensure consistency\n",
    "            current_df = current_df.select(\n",
    "                \"Line#\",\n",
    "                \"Date\",\n",
    "                \"Time\",\n",
    "                \"Water Content (m3/m3)\",\n",
    "                \"Solar Radiation (W/m2)\",\n",
    "                \"Rain (mm)\",\n",
    "                \"Temperature (Celcius)\",\n",
    "                \"RH (%)\",\n",
    "                \"Wind Speed (m/s)\",\n",
    "                \"Gust Speed (m/s)\",\n",
    "                \"Wind Direction (Degree)\",\n",
    "                \"Dew Point (Celcius)\"\n",
    "            )\n",
    "            \n",
    "            # Combine DataFrames\n",
    "            if combined_df is None:\n",
    "                combined_df = current_df\n",
    "            else:\n",
    "                combined_df = combined_df.union(current_df)\n",
    "            \n",
    "            # Clear cache after each file\n",
    "            current_df.unpersist()\n",
    "            spark.catalog.clearCache()\n",
    "            \n",
    "            print(f\"Successfully processed {file_path}\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error processing {file_path}: {str(e)}\")\n",
    "            continue\n",
    "    \n",
    "    # Write combined data to CSV\n",
    "    if combined_df is not None:\n",
    "        combined_df.coalesce(1).write \\\n",
    "            .mode(\"overwrite\") \\\n",
    "            .option(\"header\", \"true\") \\\n",
    "            .option(\"compression\", \"none\") \\\n",
    "            .csv(output_path)\n",
    "    else:\n",
    "        print(\"No data was successfully processed\")\n",
    "\n",
    "# Execute the combination process\n",
    "try:\n",
    "    process_and_combine_files(file_paths, schema, f'{MERGED_PATH}/combined_raw_data.csv')\n",
    "    print(\"Data combination completed successfully\")\n",
    "except Exception as e:\n",
    "    print(f\"Error in main process: {str(e)}\")\n",
    "finally:\n",
    "    # Clean up\n",
    "    spark.catalog.clearCache()\n",
    "    spark.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Verify the file write successfully"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+--------+-------------------+---------------------+----------------------+---------+---------------------+------+----------------+----------------+-----------------------+-------------------+\n",
      "|Line#|    Date|               Time|Water Content (m3/m3)|Solar Radiation (W/m2)|Rain (mm)|Temperature (Celcius)|RH (%)|Wind Speed (m/s)|Gust Speed (m/s)|Wind Direction (Degree)|Dew Point (Celcius)|\n",
      "+-----+--------+-------------------+---------------------+----------------------+---------+---------------------+------+----------------+----------------+-----------------------+-------------------+\n",
      "|    2|21/04/01|2024-11-30 00:05:00|               0.2532|                   1.0|      0.0|                28.02|  81.0|             0.0|             0.0|                  215.0|              24.49|\n",
      "|    3|21/04/01|2024-11-30 00:10:00|               0.2524|                   1.0|      0.0|                28.07|  81.0|             0.0|             1.3|                  170.0|              24.53|\n",
      "|    4|21/04/01|2024-11-30 00:15:00|               0.2524|                   1.0|      0.0|                 28.1|  80.8|             0.0|             1.7|                  166.0|              24.52|\n",
      "|    5|21/04/01|2024-11-30 00:20:00|               0.2524|                   1.0|      0.0|                28.07|  80.8|             0.3|             2.7|                  181.0|              24.49|\n",
      "|    6|21/04/01|2024-11-30 00:25:00|               0.2532|                   1.0|      0.0|                 28.1|  80.7|             0.0|             1.0|                  166.0|               24.5|\n",
      "+-----+--------+-------------------+---------------------+----------------------+---------+---------------------+------+----------------+----------------+-----------------------+-------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Initialize Spark session\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"Read CSV from HDFS\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# Path to the CSV file on HDFS\n",
    "file_path = f'{MERGED_PATH}/combined_raw_data.csv'\n",
    "\n",
    "# Read the CSV file into a DataFrame\n",
    "df = spark.read.csv(file_path, header=True, inferSchema=True)\n",
    "\n",
    "df.show(5)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
