{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Import necessary modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "from pyspark.sql import SparkSession\n",
    "import pandas as pd\n",
    "import warnings\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Configure the base path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_path = 'hdfs://localhost:9000/user/hadoop/input'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Function for create spark session that connect to hadoop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_spark_session():\n",
    "    \"\"\"\n",
    "    Creates and configures a SparkSession with minimal memory settings.\n",
    "    \"\"\"\n",
    "    return SparkSession.builder \\\n",
    "    .appName(\"Weather Data Combination\") \\\n",
    "    .config(\"spark.jars.packages\", \"com.crealytics:spark-excel_2.12:0.13.7\") \\\n",
    "    .config(\"spark.driver.memory\", \"2g\") \\\n",
    "    .config(\"spark.executor.memory\", \"2g\") \\\n",
    "    .config(\"spark.sql.shuffle.partitions\", \"2\") \\\n",
    "    .config(\"spark.driver.maxResultSize\", \"1g\") \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Create a function to load data from hadoop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_excel_files(spark, file_path):\n",
    "    \"\"\"\n",
    "    Load all Excel files from the given base path.\n",
    "    \"\"\"\n",
    "    df = spark.read.format(\"com.crealytics.spark.excel\") \\\n",
    "                .option(\"header\", \"false\") \\\n",
    "                .option(\"dataAddress\", \"'RUA Data'!A6\") \\\n",
    "                .option(\"maxRowsInMemory\", 1000) \\\n",
    "                .option(\"treatEmptyValuesAsNulls\", \"true\") \\\n",
    "                .load(file_path)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Test read data as xlsx file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------+--------+------+---+---+-----+----+---+---+----+-----+\n",
      "|_c0|     _c1|     _c2|   _c3|_c4|_c5|  _c6| _c7|_c8|_c9|_c10| _c11|\n",
      "+---+--------+--------+------+---+---+-----+----+---+---+----+-----+\n",
      "|  1|21/04/01|00:00:00|0.2534|  1|  0|28.12|80.6|  0|0.3| 149| 24.5|\n",
      "|  2|21/04/01|00:05:00|0.2532|  1|  0|28.02|  81|  0|  0| 215|24.49|\n",
      "|  3|21/04/01|00:10:00|0.2524|  1|  0|28.07|  81|  0|1.3| 170|24.53|\n",
      "|  4|21/04/01|00:15:00|0.2524|  1|  0| 28.1|80.8|  0|1.7| 166|24.52|\n",
      "|  5|21/04/01|00:20:00|0.2524|  1|  0|28.07|80.8|0.3|2.7| 181|24.49|\n",
      "+---+--------+--------+------+---+---+-----+----+---+---+----+-----+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark = create_spark_session()\n",
    "hdfs_path = f'{base_path}/APRIL-2021.xlsx'\n",
    "df = load_excel_files(spark, hdfs_path)\n",
    "df.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Get all datanode that we work with"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['hdfs://localhost:9000/user/hadoop/input/APRIL-2021.xlsx', 'hdfs://localhost:9000/user/hadoop/input/APRIL-2022.xlsx', 'hdfs://localhost:9000/user/hadoop/input/AUGUST-2021.xlsx', 'hdfs://localhost:9000/user/hadoop/input/DECEMBER-2020.xlsx', 'hdfs://localhost:9000/user/hadoop/input/DECEMBER-2021.xlsx', 'hdfs://localhost:9000/user/hadoop/input/FEBRUARY-2021.xlsx', 'hdfs://localhost:9000/user/hadoop/input/FEBRUARY-2022.xlsx', 'hdfs://localhost:9000/user/hadoop/input/JANUARY-2021.xlsx', 'hdfs://localhost:9000/user/hadoop/input/JANUARY-2022.xlsx', 'hdfs://localhost:9000/user/hadoop/input/JULY-2021.xlsx', 'hdfs://localhost:9000/user/hadoop/input/MARCH-2021.xlsx', 'hdfs://localhost:9000/user/hadoop/input/MARCH-2022.xlsx', 'hdfs://localhost:9000/user/hadoop/input/MAY-2021.xlsx', 'hdfs://localhost:9000/user/hadoop/input/NOVEMBER-2020.xlsx', 'hdfs://localhost:9000/user/hadoop/input/NOVEMBER-2021.xlsx', 'hdfs://localhost:9000/user/hadoop/input/OCTOBER-2020.xlsx', 'hdfs://localhost:9000/user/hadoop/input/OCTOBER-2021.xlsx', 'hdfs://localhost:9000/user/hadoop/input/SEPTEMBER-2020.xlsx', 'hdfs://localhost:9000/user/hadoop/input/SEPTEMBER-2021.xlsx', 'hdfs://localhost:9000/user/hadoop/input/jUNE-2021.xlsx']\n"
     ]
    }
   ],
   "source": [
    "file_paths = [f\"{base_path}/{filename}\" for filename in [\n",
    "    \"APRIL-2021.xlsx\", \"APRIL-2022.xlsx\", \"AUGUST-2021.xlsx\",\n",
    "    \"DECEMBER-2020.xlsx\", \"DECEMBER-2021.xlsx\", \"FEBRUARY-2021.xlsx\",\n",
    "    \"FEBRUARY-2022.xlsx\", \"JANUARY-2021.xlsx\", \"JANUARY-2022.xlsx\",\n",
    "    \"JULY-2021.xlsx\", \"MARCH-2021.xlsx\", \"MARCH-2022.xlsx\",\n",
    "    \"MAY-2021.xlsx\", \"NOVEMBER-2020.xlsx\", \"NOVEMBER-2021.xlsx\",\n",
    "    \"OCTOBER-2020.xlsx\", \"OCTOBER-2021.xlsx\", \"SEPTEMBER-2020.xlsx\",\n",
    "    \"SEPTEMBER-2021.xlsx\", \"jUNE-2021.xlsx\"\n",
    "]]\n",
    "print(file_paths)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convert the excel file to dataframe and append it to an array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading file: hdfs://localhost:9000/user/hadoop/input/APRIL-2021.xlsx\n",
      "Reading file: hdfs://localhost:9000/user/hadoop/input/APRIL-2022.xlsx\n",
      "Reading file: hdfs://localhost:9000/user/hadoop/input/AUGUST-2021.xlsx\n",
      "Reading file: hdfs://localhost:9000/user/hadoop/input/DECEMBER-2020.xlsx\n",
      "Reading file: hdfs://localhost:9000/user/hadoop/input/DECEMBER-2021.xlsx\n",
      "Reading file: hdfs://localhost:9000/user/hadoop/input/FEBRUARY-2021.xlsx\n",
      "Reading file: hdfs://localhost:9000/user/hadoop/input/FEBRUARY-2022.xlsx\n",
      "Reading file: hdfs://localhost:9000/user/hadoop/input/JANUARY-2021.xlsx\n",
      "Reading file: hdfs://localhost:9000/user/hadoop/input/JANUARY-2022.xlsx\n",
      "Reading file: hdfs://localhost:9000/user/hadoop/input/JULY-2021.xlsx\n",
      "Reading file: hdfs://localhost:9000/user/hadoop/input/MARCH-2021.xlsx\n",
      "Reading file: hdfs://localhost:9000/user/hadoop/input/MARCH-2022.xlsx\n",
      "Reading file: hdfs://localhost:9000/user/hadoop/input/MAY-2021.xlsx\n",
      "Reading file: hdfs://localhost:9000/user/hadoop/input/NOVEMBER-2020.xlsx\n",
      "Reading file: hdfs://localhost:9000/user/hadoop/input/NOVEMBER-2021.xlsx\n",
      "Reading file: hdfs://localhost:9000/user/hadoop/input/OCTOBER-2020.xlsx\n",
      "Reading file: hdfs://localhost:9000/user/hadoop/input/OCTOBER-2021.xlsx\n",
      "Reading file: hdfs://localhost:9000/user/hadoop/input/SEPTEMBER-2020.xlsx\n",
      "Reading file: hdfs://localhost:9000/user/hadoop/input/SEPTEMBER-2021.xlsx\n",
      "Reading file: hdfs://localhost:9000/user/hadoop/input/jUNE-2021.xlsx\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "# Initialize list for Pandas DataFrames\n",
    "excl_list = []\n",
    "\n",
    "# Read Excel files\n",
    "for file in file_paths:\n",
    "    try:\n",
    "        print(f\"Reading file: {file}\")\n",
    "        # Read the file into a Spark DataFrame\n",
    "        spark_df = load_excel_files(spark=spark, file_path=file)\n",
    "\n",
    "        # Convert Spark DataFrame to Pandas DataFrame\n",
    "        excl_list.append(spark_df.toPandas())\n",
    "    except Exception as e:\n",
    "        print(f\"Error reading {file}: {e}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test Show the append array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>_c0</th>\n",
       "      <th>_c1</th>\n",
       "      <th>_c2</th>\n",
       "      <th>_c3</th>\n",
       "      <th>_c4</th>\n",
       "      <th>_c5</th>\n",
       "      <th>_c6</th>\n",
       "      <th>_c7</th>\n",
       "      <th>_c8</th>\n",
       "      <th>_c9</th>\n",
       "      <th>_c10</th>\n",
       "      <th>_c11</th>\n",
       "      <th>_c12</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>21/04/01</td>\n",
       "      <td>00:00:00</td>\n",
       "      <td>0.2534</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>28.12</td>\n",
       "      <td>80.6</td>\n",
       "      <td>0</td>\n",
       "      <td>0.3</td>\n",
       "      <td>149</td>\n",
       "      <td>24.5</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>21/04/01</td>\n",
       "      <td>00:05:00</td>\n",
       "      <td>0.2532</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>28.02</td>\n",
       "      <td>81</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>215</td>\n",
       "      <td>24.49</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>21/04/01</td>\n",
       "      <td>00:10:00</td>\n",
       "      <td>0.2524</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>28.07</td>\n",
       "      <td>81</td>\n",
       "      <td>0</td>\n",
       "      <td>1.3</td>\n",
       "      <td>170</td>\n",
       "      <td>24.53</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>21/04/01</td>\n",
       "      <td>00:15:00</td>\n",
       "      <td>0.2524</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>28.1</td>\n",
       "      <td>80.8</td>\n",
       "      <td>0</td>\n",
       "      <td>1.7</td>\n",
       "      <td>166</td>\n",
       "      <td>24.52</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>21/04/01</td>\n",
       "      <td>00:20:00</td>\n",
       "      <td>0.2524</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>28.07</td>\n",
       "      <td>80.8</td>\n",
       "      <td>0.3</td>\n",
       "      <td>2.7</td>\n",
       "      <td>181</td>\n",
       "      <td>24.49</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  _c0       _c1       _c2     _c3 _c4 _c5    _c6   _c7  _c8  _c9 _c10   _c11  \\\n",
       "0   1  21/04/01  00:00:00  0.2534   1   0  28.12  80.6    0  0.3  149   24.5   \n",
       "1   2  21/04/01  00:05:00  0.2532   1   0  28.02    81    0    0  215  24.49   \n",
       "2   3  21/04/01  00:10:00  0.2524   1   0  28.07    81    0  1.3  170  24.53   \n",
       "3   4  21/04/01  00:15:00  0.2524   1   0   28.1  80.8    0  1.7  166  24.52   \n",
       "4   5  21/04/01  00:20:00  0.2524   1   0  28.07  80.8  0.3  2.7  181  24.49   \n",
       "\n",
       "  _c12  \n",
       "0  NaN  \n",
       "1  NaN  \n",
       "2  NaN  \n",
       "3  NaN  \n",
       "4  NaN  "
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "combined_df = pd.concat(excl_list, ignore_index=True)\n",
    "# Display the first five rows\n",
    "combined_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Let check the col . contain null value , it seem like have a col. contain all null value let drop it\n",
    "#### And drop col. (_c0) that we no need"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_c0          0\n",
      "_c1          0\n",
      "_c2          0\n",
      "_c3          0\n",
      "_c4          0\n",
      "_c5          0\n",
      "_c6          0\n",
      "_c7          0\n",
      "_c8          0\n",
      "_c9          0\n",
      "_c10         0\n",
      "_c11         0\n",
      "_c12    160438\n",
      "dtype: int64\n",
      "len_c12 160438\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>_c1</th>\n",
       "      <th>_c2</th>\n",
       "      <th>_c3</th>\n",
       "      <th>_c4</th>\n",
       "      <th>_c5</th>\n",
       "      <th>_c6</th>\n",
       "      <th>_c7</th>\n",
       "      <th>_c8</th>\n",
       "      <th>_c9</th>\n",
       "      <th>_c10</th>\n",
       "      <th>_c11</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>21/04/01</td>\n",
       "      <td>00:00:00</td>\n",
       "      <td>0.2534</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>28.12</td>\n",
       "      <td>80.6</td>\n",
       "      <td>0</td>\n",
       "      <td>0.3</td>\n",
       "      <td>149</td>\n",
       "      <td>24.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>21/04/01</td>\n",
       "      <td>00:05:00</td>\n",
       "      <td>0.2532</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>28.02</td>\n",
       "      <td>81</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>215</td>\n",
       "      <td>24.49</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>21/04/01</td>\n",
       "      <td>00:10:00</td>\n",
       "      <td>0.2524</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>28.07</td>\n",
       "      <td>81</td>\n",
       "      <td>0</td>\n",
       "      <td>1.3</td>\n",
       "      <td>170</td>\n",
       "      <td>24.53</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>21/04/01</td>\n",
       "      <td>00:15:00</td>\n",
       "      <td>0.2524</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>28.1</td>\n",
       "      <td>80.8</td>\n",
       "      <td>0</td>\n",
       "      <td>1.7</td>\n",
       "      <td>166</td>\n",
       "      <td>24.52</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>21/04/01</td>\n",
       "      <td>00:20:00</td>\n",
       "      <td>0.2524</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>28.07</td>\n",
       "      <td>80.8</td>\n",
       "      <td>0.3</td>\n",
       "      <td>2.7</td>\n",
       "      <td>181</td>\n",
       "      <td>24.49</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        _c1       _c2     _c3 _c4 _c5    _c6   _c7  _c8  _c9 _c10   _c11\n",
       "0  21/04/01  00:00:00  0.2534   1   0  28.12  80.6    0  0.3  149   24.5\n",
       "1  21/04/01  00:05:00  0.2532   1   0  28.02    81    0    0  215  24.49\n",
       "2  21/04/01  00:10:00  0.2524   1   0  28.07    81    0  1.3  170  24.53\n",
       "3  21/04/01  00:15:00  0.2524   1   0   28.1  80.8    0  1.7  166  24.52\n",
       "4  21/04/01  00:20:00  0.2524   1   0  28.07  80.8  0.3  2.7  181  24.49"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(combined_df.isnull().sum())\n",
    "print('len_c12' , len(combined_df['_c12']))\n",
    "# axis == 0 for row and 1 for col.\n",
    "combined_df.drop(['_c12' ,'_c0'] , axis=1 , inplace=True)\n",
    "combined_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Renaming certain columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>time</th>\n",
       "      <th>water_content</th>\n",
       "      <th>solar_radiation</th>\n",
       "      <th>rain</th>\n",
       "      <th>temperature</th>\n",
       "      <th>rh</th>\n",
       "      <th>wind_speed</th>\n",
       "      <th>gust_speed</th>\n",
       "      <th>wind_direction</th>\n",
       "      <th>dew_point</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>21/04/01</td>\n",
       "      <td>00:00:00</td>\n",
       "      <td>0.2534</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>28.12</td>\n",
       "      <td>80.6</td>\n",
       "      <td>0</td>\n",
       "      <td>0.3</td>\n",
       "      <td>149</td>\n",
       "      <td>24.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>21/04/01</td>\n",
       "      <td>00:05:00</td>\n",
       "      <td>0.2532</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>28.02</td>\n",
       "      <td>81</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>215</td>\n",
       "      <td>24.49</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>21/04/01</td>\n",
       "      <td>00:10:00</td>\n",
       "      <td>0.2524</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>28.07</td>\n",
       "      <td>81</td>\n",
       "      <td>0</td>\n",
       "      <td>1.3</td>\n",
       "      <td>170</td>\n",
       "      <td>24.53</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>21/04/01</td>\n",
       "      <td>00:15:00</td>\n",
       "      <td>0.2524</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>28.1</td>\n",
       "      <td>80.8</td>\n",
       "      <td>0</td>\n",
       "      <td>1.7</td>\n",
       "      <td>166</td>\n",
       "      <td>24.52</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>21/04/01</td>\n",
       "      <td>00:20:00</td>\n",
       "      <td>0.2524</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>28.07</td>\n",
       "      <td>80.8</td>\n",
       "      <td>0.3</td>\n",
       "      <td>2.7</td>\n",
       "      <td>181</td>\n",
       "      <td>24.49</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       date      time water_content solar_radiation rain temperature    rh  \\\n",
       "0  21/04/01  00:00:00        0.2534               1    0       28.12  80.6   \n",
       "1  21/04/01  00:05:00        0.2532               1    0       28.02    81   \n",
       "2  21/04/01  00:10:00        0.2524               1    0       28.07    81   \n",
       "3  21/04/01  00:15:00        0.2524               1    0        28.1  80.8   \n",
       "4  21/04/01  00:20:00        0.2524               1    0       28.07  80.8   \n",
       "\n",
       "  wind_speed gust_speed wind_direction dew_point  \n",
       "0          0        0.3            149      24.5  \n",
       "1          0          0            215     24.49  \n",
       "2          0        1.3            170     24.53  \n",
       "3          0        1.7            166     24.52  \n",
       "4        0.3        2.7            181     24.49  "
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = combined_df.rename(columns={\n",
    "    '_c1': 'date',\n",
    "    '_c2': 'time',\n",
    "    '_c3': 'water_content',\n",
    "    '_c4': 'solar_radiation',\n",
    "    '_c5': 'rain',\n",
    "    '_c6': 'temperature',\n",
    "    '_c7': 'rh',\n",
    "    '_c8': 'wind_speed',\n",
    "    '_c9': 'gust_speed',\n",
    "    '_c10': 'wind_direction',\n",
    "    '_c11': 'dew_point',\n",
    "})\n",
    "df.head(5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Outliers based on Z-score:\n",
      " Empty DataFrame\n",
      "Columns: [date, time, water_content, z_score]\n",
      "Index: []\n"
     ]
    }
   ],
   "source": [
    "Q1 = df['water_content'].quantile(0.25)\n",
    "Q3 = df['water_content'].quantile(0.75)\n",
    "IQR = Q3 - Q1\n",
    "\n",
    "# Find outliers based on IQR\n",
    "outliers_iqr = df[(df['water_content'] < (Q1 - 1.5 * IQR)) | (df['water_content'] > (Q3 + 1.5 * IQR))]\n",
    "print(\"Outliers based on IQR:\\n\", outliers_iqr)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
